This repository contains all important files to reproduce the results of the master thesis "A Survey on Automatic Text Simplification for German".

# Content

## Notebooks
Here you can find all the notebooks used to prepare the data, train the models and then evaluate them.
The notebooks that need to access the corpus data download it as a separate repository.
If you run the notebooks on google colab you have to adjust the path of the prefixes.

The numbering of the notebooks should help you to execute them in the right order. 
For example 02_finetune_decoder or 03_pretraining should always be executed before 04_finetuning, because 04 needs the trained models of 02 or 03.
However, you can also completely skip experiments 02 and 03 for the data augmentation experiments.

The easiest way to implement MBart models with custom decoder is to use the EncoderDecoderModel from huggingface:
```python
#load pre-trained models into one EncoderDecoder
mbart = MBartModel.from_pretrained(mbart_path)
decoder = AutoModelForCausalLM.from_pretrained(decoder_path)
model = EncoderDecoderModel(encoder=mbart.encoder,decoder=decoder)

#push model
model.push_to_hub("custom-mbart")

#load fine-tuned model again
model = EncoderDecoderModel.from_pretrained("custom-mbart") #encoder weights are newly initialized as MBartEncoder is no official encoder architecture
mbart =  MBartModel.from_pretrained("custom-mbart", config=model.config.encoder) #wrap the custom-mbart weights into a full MBartModel
model.encoder = mbart.encoder #set the EncoderDecoders encoder to the fine-tuned version
```

## Results
This folder contains .txt files with the translations of the respective models.
It is divided into the different main groups of experiments.
- data-augmentation
- finetuned mbart decoders
- pre-training of cross-attentions

Each of these folders contains a subfolder that contains either the results for the 20min or the Kurier dataset, or both. The actual experiments are contained in their subfolders and consist of several files with possible translations generated by the Monte Carlo method.

# Questions?
Ask: joshua.oehms@tum.de

