{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJgZ0hcMul3Z"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#download the data\n",
    "!git clone https://dev:dtKN5sX9We7pw1soPB19@gitlab.lrz.de/josh-o/leichte-sprache-corpus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og-eOzlYvc_4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#install dependencies (April 14, 2023)\n",
    "#pytorch 2.0.0+cu118\n",
    "#Python 3.9.16\n",
    "!pip install transformers==4.28.0 \n",
    "!pip install sentencepiece==0.1.98\n",
    "!pip install sacremoses==0.0.53\n",
    "!pip install evaluate==0.3.0\n",
    "!pip install sacrebleu==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvHNh39Uveu-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import set_seed\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = 100\n",
    "set_seed(seed) # no direct effect on text generation\n",
    "\n",
    "PREFIX = \"../../leichte-sprache-corpus/aligned/20min/\"\n",
    "PREFIX_KURIER = \"../../leichte-sprache-corpus/aligned/kurier/\"\n",
    "PREFIX_AUGMENTED = \"../../leichte-sprache-corpus/aligned/20min/augmented/\"\n",
    "PREFIX_AUGMENTED_KURIER = \"../../leichte-sprache-corpus/aligned/kurier/augmented/\"\n",
    "\n",
    "model_path, revision, decoder_path, experiment_name, n = (None, None, None, None, None)\n",
    "\n",
    "                #20min dataset size impact\n",
    "                #TODO\n",
    "\n",
    "    \n",
    "                #Kurier monolingual fine-tuning\n",
    "    \n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"c3ec8bd0c64fcec8390c4816c03c895809708f05\" , \"Kurier-mbart-0_1-0_1\", 5)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"676e21a3c44b98bfacc40ae294a1971e951e5215\" , \"Kurier-ft-decoder-mbart-0_1-0_1\", 5)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"8c10a5dd536a02409f2ff9cb61ed37688ffc27b0\" , \"Kurier-ft-decoder-gaussian-noise-mbart-0_1-0_1\", 5)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"6be4e7d6e269487672917a4663d8570928562fff\" , \"Kurier-ft-decoder-bart-noise-mbart-0_1-0_1\", 5)\n",
    "\n",
    "                #20min pre-training\n",
    "        \n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"f36e16fde71a91062cfb9c19de4005a03b1d7af5\" , \"20min-regular-mbart-0_1-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"5e6dffaeb9c08d38579378621e6db1d6d69dd0bc\" , \"20min-random-mbart-0_1-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"6d4ece67a7d52fad9aa8f818368475e79cfbf429\" , \"20min-retrained-mbart-0_1-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"8aa7d496e900a85fd025ad85d1263990e311f65b\" , \"20min-distilled[r,r,-1]-mbart-0_1-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts\", \"a854fb829e1eed3f37a79f152ec3048a6e438b25\" , \"20min-distilled[-1]-mbart-0_1-0_1\", 10)\n",
    "\n",
    "#model_path, revision, decoder_path, experiment_name, n = (\"josh-oo/mbart-custom\", \"89e91843fb2bbf481fdc765b1d8628669f63aac4\", \"josh-oo/german-gpt2-easy\", \"20min-one-to-one-gpt-0_1-0_1\", 10)\n",
    "model_path, revision, decoder_path, experiment_name, n = (\"josh-oo/mbart-custom\", \"08c228b7d05b2bc1583b02c5c32af23b681272b2\", \"josh-oo/german-gpt2-easy\", \"20min-bart-noise-gpt-0_1-0_1\", 10)\n",
    "\n",
    "                #20min simple data augmentation training\n",
    "\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"67950c73adee6d6428748c98340c22b5afb25cb5\" , \"20min-dropout-0_0-0_1\", 10)\n",
    "\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"ddc150b84001af02e9937f014ec085575f4362b5\" , \"20min-dropout-0_1-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"580548b2d0dd036e02f8d5f2ee282ec058424c71\" , \"20min-backtranslation-0_0-0_1\", 10)\n",
    "\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"4c9641e03f3285c28e165ae39c729049554e7b4c\" , \"20min-dropout-0_3-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"365bd895ec2533e03b01c9d2e388653382fa8873\" , \"20min-simple_noise-0_0-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"75aa9b327e58a82716aaece9ea9bf61fa178f960\" , \"20min-bart_noise-0_0-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"07fb2e359bd22a71ca6f633759b31b245f632d65\" , \"20min-bt_noise-0_0-0_1\", 10)\n",
    "\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"a42fff7cfb0fee553aba80e1690ca8135251377d\" , \"20min-dropout-0_8-0_1\", 10)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"4993499db62917d463b428e346957e8fdb7d31a7\" , \"20min-english-0_0-0_1\", 10)\n",
    "\n",
    "                #Kurier simple data augmentation training\n",
    "    \n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"5891aa82b21a7f0c0b972bd42afd185f6aee0ad5\" , \"Kurier-dropout-0_0-0_1\", 5)\n",
    "\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"d11d747e36b5b718f2b9675a30216aa2f7fc8245\" , \"Kurier-dropout-0_1-0_1\", 5)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"e4e338fce0853175166df88c0bfbf2bf4dc86792\" , \"Kurier-backtranslation-0_0-0_1\", 5)\n",
    "\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"b51eba22c28c346b0efaa7776f970785952e37bc\" , \"Kurier-dropout-0_3-0_1\", 5)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"163ffb88f6c460758c9782df34181206321c6af4\" , \"Kurier-simple_noise-0_0-0_1\", 5)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"de1ff6851f1926029766daf80ce96cd09924c670\" , \"Kurier-bart_noise-0_0-0_1\", 5)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"628867e037440a357760c0230554a636baa46a8b\" , \"Kurier-bt_noise-0_0-0_1\", 5)\n",
    "\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"40eec281bf461cf96b872dfa8f57ee33669590ae\" , \"Kurier-dropout-0_8-0_1\", 5)\n",
    "#model_path, revision, experiment_name, n = (\"josh-oo/mbart-ts-da\", \"c818a09192431e359dfbe6b810c70af27a7c35cb\" , \"Kurier-english-0_0-0_1\", 5)\n",
    "\n",
    "\n",
    "number_of_samples = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if decoder_path is not None:\n",
    "    from transformers import  PreTrainedModel\n",
    "    from transformers import EncoderDecoderModel, EncoderDecoderConfig, MBartTokenizerFast, AutoModelForCausalLM, GPT2TokenizerFast\n",
    "    from transformers.models.mbart.modeling_mbart import  MBartEncoder, MBartModel\n",
    "            \n",
    "    class EncoderWrapper(PreTrainedModel):\n",
    "        def __init__(self, config):\n",
    "            super().__init__(config)\n",
    "            self.encoder = MBartEncoder(config)\n",
    "    \n",
    "    class MonteCarloWrapper(EncoderDecoderModel):\n",
    "\n",
    "        def forward(self,input_ids = None, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None,\n",
    "              encoder_outputs = None, past_key_values = None, inputs_embeds = None, decoder_inputs_embeds = None, labels = None, \n",
    "              use_cache = None, output_attentions = None, output_hidden_states = None, return_dict = None):\n",
    "                #unfortunately we have to copy all arguments as the .generate method internally uses inspect\n",
    "                #to remove columns not used in the functions argumenst\n",
    "\n",
    "            self.get_decoder().train()#enable monte carlo dropout\n",
    "            #disable trainable batchnorm\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.eval()\n",
    "\n",
    "            return super().forward(input_ids = input_ids, attention_mask = attention_mask, decoder_input_ids = decoder_input_ids,\n",
    "                           decoder_attention_mask = decoder_attention_mask, encoder_outputs = encoder_outputs, \n",
    "                           past_key_values = past_key_values, inputs_embeds = inputs_embeds, decoder_inputs_embeds = decoder_inputs_embeds,\n",
    "                           labels = labels, use_cache = use_cache, output_attentions = output_attentions, output_hidden_states = output_hidden_states, \n",
    "                           return_dict = return_dict)\n",
    "\n",
    "    model = MonteCarloWrapper.from_pretrained(\"josh-oo/mbart-custom\")\n",
    "    \n",
    "    #workaround to load mbart encoder into the encoder-decoder model\n",
    "    mbart_encoder =  MBartModel.from_pretrained(\"josh-oo/mbart-custom\", config=model.config.encoder)\n",
    "    model.encoder = mbart_encoder.encoder\n",
    "\n",
    "    input_tokenizer = MBartTokenizerFast.from_pretrained(\"facebook/mbart-large-cc25\", src_lang=\"de_DE\", tgt_lang=\"de_DE\")\n",
    "    output_tokenizer = GPT2TokenizerFast.from_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFQe6W9Bv2fc"
   },
   "outputs": [],
   "source": [
    "if decoder_path is None:\n",
    "    from transformers import MBartForConditionalGeneration, MBartTokenizerFast, MBartConfig\n",
    "    class MonteCarloWrapper(MBartForConditionalGeneration):\n",
    "\n",
    "        def forward(self,input_ids = None,attention_mask = None,decoder_input_ids = None,\n",
    "                  decoder_attention_mask = None,head_mask = None,decoder_head_mask = None,\n",
    "                  cross_attn_head_mask = None,encoder_outputs = None,past_key_values = None,inputs_embeds = None,\n",
    "                  decoder_inputs_embeds = None,labels = None,use_cache = None,output_attentions = None,\n",
    "                  output_hidden_states = None,return_dict = None):\n",
    "                    #unfortunately we have to copy all arguments as the .generate method internally uses inspect\n",
    "                    #to remove columns not used in the functions argumenst\n",
    "\n",
    "            self.model.get_decoder().train()#enable monte carlo dropout\n",
    "            #disable trainable batchnorm\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, torch.nn.LayerNorm):\n",
    "                    module.eval()\n",
    "\n",
    "            return super().forward(input_ids = input_ids,attention_mask = attention_mask,decoder_input_ids = decoder_input_ids,\n",
    "                  decoder_attention_mask = decoder_attention_mask,head_mask = head_mask,decoder_head_mask = decoder_head_mask,\n",
    "                  cross_attn_head_mask = cross_attn_head_mask,encoder_outputs = encoder_outputs,past_key_values = past_key_values,inputs_embeds = inputs_embeds,\n",
    "                  decoder_inputs_embeds = decoder_inputs_embeds,labels = labels,use_cache = use_cache,output_attentions = output_attentions,\n",
    "                  output_hidden_states = output_hidden_states,return_dict = return_dict)\n",
    "\n",
    "    model_config = MBartConfig.from_pretrained(model_path)\n",
    "    model_config.dropout = 0.1\n",
    "\n",
    "    model = MonteCarloWrapper.from_pretrained(model_path, config=model_config, revision=revision)\n",
    "\n",
    "    tokenizer = MBartTokenizerFast.from_pretrained(\"facebook/mbart-large-cc25\", src_lang=\"de_DE\", tgt_lang=\"de_DE\")\n",
    "    input_tokenizer, output_tokenizer = tokenizer,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3o9bdhPwv5St"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,concatenate_datasets, Features, Value\n",
    "import unicodedata\n",
    "\n",
    "def normalize(text):\n",
    "  text['normal_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['normal_phrase'].strip())\n",
    "  text['simple_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['simple_phrase'].strip())\n",
    "  return text\n",
    "\n",
    "def tokenize(text, input_tokenizer, output_tokenizer, max_input_length):\n",
    "  inputs = input_tokenizer(text[\"normal_phrase\"], return_tensors=\"np\")\n",
    "  labels = output_tokenizer(text[\"simple_phrase\"], return_tensors=\"np\", truncation=True, max_length=max_input_length)\n",
    "  inputs['labels'] = labels['input_ids']\n",
    "  return inputs\n",
    "\n",
    "def get_dataset(data_files, input_tokenizer, output_tokenizer, name=None, max_input_length=None, seed=None):\n",
    "  features = Features({'normal_phrase': Value('string'), 'simple_phrase': Value('string')})\n",
    "\n",
    "  data = load_dataset(\"csv\",name=name, data_files=data_files, features=features)\n",
    "  data = data.map(normalize, num_proc=4)\n",
    "  data = data.map(lambda rows: tokenize(rows, input_tokenizer, output_tokenizer, max_input_length), batched=True)\n",
    "  if \"train\" in data:\n",
    "    data['train'] = data['train'].map(count, num_proc=4)\n",
    "    data = data.remove_columns([column for column in data.column_names['train'] if column not in ['labels','input_ids','attention_mask']])\n",
    "  else:\n",
    "    data = data.remove_columns([column for column in data.column_names['test'] if column not in ['labels','input_ids','attention_mask']])\n",
    "\n",
    "  if max_input_length is not None:\n",
    "    data = data.filter(lambda example: len(example[\"input_ids\"]) < max_input_length)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m37COzJ1v8kp"
   },
   "outputs": [],
   "source": [
    "if  experiment_name.startswith(\"20min\"):\n",
    "    data_files_20_min = {'test': PREFIX + \"20min_aligned_test.csv\"}\n",
    "    data_train = get_dataset(data_files_20_min, input_tokenizer, output_tokenizer, \"20min\", max_input_length=model.config.max_length,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYv9i8SKjg0_"
   },
   "outputs": [],
   "source": [
    "if  experiment_name.startswith(\"Kurier\"):\n",
    "    data_files_kurier = {'test': PREFIX_KURIER + \"kurier_aligned_test.csv\"}\n",
    "    data_train = get_dataset(data_files_kurier, input_tokenizer, output_tokenizer, \"kurier\", max_input_length=model.config.max_length,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW0Msbw4v-xD"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from collections.abc import Iterable\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "sari = evaluate.load(\"sari\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "class TestTrainer(Seq2SeqTrainer):\n",
    "  def evaluate(self,eval_dataset = None,ignore_keys = None,metric_key_prefix = \"eval\",**gen_kwargs):\n",
    "    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix, num_beams=1, do_sample=False)\n",
    "\n",
    "  def predict(self,test_dataset,ignore_keys = None,metric_key_prefix = \"test\",**gen_kwargs):\n",
    "    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix, num_beams=3, do_sample=False)\n",
    "\n",
    "def compute_translation_metrics(input_tokenizer, output_tokenizer, pred, control_tokens):\n",
    "\n",
    "    input_ids = pred.inputs\n",
    "    label_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    input_ids[input_ids == -100] = input_tokenizer.pad_token_id\n",
    "    label_ids[label_ids == -100] = output_tokenizer.pad_token_id\n",
    "    pred_ids[pred_ids == -100] = output_tokenizer.pad_token_id\n",
    "\n",
    "    input_str_list = input_tokenizer.batch_decode(input_ids, skip_special_tokens=True,\n",
    "                                                  clean_up_tokenization_spaces=False)\n",
    "    pred_str_list = output_tokenizer.batch_decode(pred_ids, skip_special_tokens=True,\n",
    "                                                  clean_up_tokenization_spaces=False)\n",
    "    label_str_list = output_tokenizer.batch_decode(label_ids, skip_special_tokens=True,\n",
    "                                                   clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    if control_tokens == True:\n",
    "      for i in range(0, len(input_str_list)):\n",
    "        input_str_list[i] = input_str_list[i].split(' ', 1)[1]\n",
    "\n",
    "    label_str_list = [[label] for label in label_str_list]\n",
    "\n",
    "\n",
    "    sari_score = sari.compute(sources=input_str_list, predictions=pred_str_list, references=label_str_list)\n",
    "    bleu_score = bleu.compute(predictions=pred_str_list, references=label_str_list)\n",
    "\n",
    "    translation_result = {\n",
    "        'sari':sari_score['sari'],\n",
    "        'bleu':bleu_score['bleu']*100\n",
    "    }\n",
    "\n",
    "    return {key: sum(value) / len(value) if isinstance(value, Iterable) else value for (key, value) in\n",
    "            translation_result.items()}\n",
    "\n",
    "compute_metrics = lambda pred: compute_translation_metrics(input_tokenizer, output_tokenizer, pred, control_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BxdbJPfwDQK"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=input_tokenizer, model=model, pad_to_multiple_of=8)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=True,\n",
    "    generation_max_length=1024,\n",
    "    output_dir=\"../results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy='no',\n",
    "    per_device_eval_batch_size=4, \n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    group_by_length=True,\n",
    "    seed=700,\n",
    "    data_seed=seed,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_num_workers=2,\n",
    "    optim='adamw_torch',\n",
    ")\n",
    "\n",
    "trainer = TestTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics = compute_metrics,\n",
    "    train_dataset=data_train['test'],\n",
    "    eval_dataset=data_train['test'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlxyCpAnwGDh"
   },
   "outputs": [],
   "source": [
    "def save_preds(preds, name):\n",
    "  outputs = output_tokenizer.batch_decode(preds.predictions, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    "  for i in range(0,len(outputs)):\n",
    "    outputs[i] = outputs[i].replace(\"\\n\", \" \")\n",
    "  with open(name + \".txt\", 'w') as fp:\n",
    "    fp.write(\"\\n\".join(outputs))\n",
    "    print(name + ' Done!')\n",
    "\n",
    "for i in range(0,number_of_samples):\n",
    "  preds = trainer.predict(test_dataset=data_train['test'])\n",
    "  save_preds(preds, experiment_name + \"-\" + str(i))\n",
    "  print(preds.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh6irpT51qYs"
   },
   "outputs": [],
   "source": [
    "#select median, best and worst translation according to BLEU\n",
    "#select the most representative text according to ROUGE\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "concatenated = []\n",
    "\n",
    "best_predictions = []\n",
    "worst_predictions = []\n",
    "median_predictions = []\n",
    "representing_predictions = []\n",
    "\n",
    "target_experiment = experiment_name\n",
    "\n",
    "for txt_file in os.listdir(\".\"):\n",
    "  if txt_file.startswith(target_experiment) and txt_file.endswith(\".txt\") and txt_file.replace(\".txt\", \"\").split(\"-\")[-1].isnumeric():\n",
    "    with open(txt_file) as file:\n",
    "      predictions = [line.rstrip() for line in file]\n",
    "      all_predictions.append(predictions)\n",
    "\n",
    "all_references = []\n",
    "all_sources = []\n",
    "for i in range(0, len(data_train['test'])):\n",
    "  reference = output_tokenizer.decode(data_train['test'][i]['labels'], clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    "  source = input_tokenizer.decode(data_train['test'][i]['input_ids'], clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    " \n",
    "  all_references.append(reference)\n",
    "  all_sources.append(source)\n",
    "  bleu_scores = []\n",
    "  for a, prediction in enumerate(all_predictions):\n",
    "    current_pred = prediction[i]\n",
    "    bleu_score = 0\n",
    "    if len(current_pred) != 0:  \n",
    "        bleu_score = bleu.compute(predictions=[current_pred], references=[[reference]])\n",
    "        bleu_scores.append(bleu_score['bleu']*100)\n",
    "  \n",
    "  best_prediction = all_predictions[np.argmax(bleu_scores)][i]\n",
    "  best_predictions.append(best_prediction)\n",
    "  \n",
    "  worst_prediction = all_predictions[np.argmin(bleu_scores)][i]\n",
    "  worst_predictions.append(worst_prediction)\n",
    "\n",
    "  pseudo_median = np.argmin(np.abs(bleu_scores - np.median(bleu_scores)))\n",
    "  median_prediction = all_predictions[pseudo_median][i]\n",
    "  median_predictions.append(median_prediction)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "cache = {}\n",
    "for i, prediction_file in tqdm(enumerate(all_predictions)):\n",
    "  for a, counter_prediction_file in enumerate(all_predictions):\n",
    "    if a == i:\n",
    "      continue\n",
    "    scores = cache.get(str(min(i,a)) + \"-\" + str(max(i,a)), None)\n",
    "    if scores == None:\n",
    "      scores = rouge.compute(predictions=prediction_file,references=counter_prediction_file, use_aggregator=False)['rougeL']\n",
    "      cache[str(min(i,a)) + \"-\" + str(max(i,a))] = scores\n",
    "\n",
    "number_of_candidates = len(all_predictions)\n",
    "number_of_tests = len(all_predictions[0])\n",
    "for i in range(0, number_of_tests):\n",
    "  mean_metrics = []\n",
    "  for a in range(0, number_of_candidates):\n",
    "    summed_metric = 0\n",
    "    for b in range(0, number_of_candidates):\n",
    "      if a != b:\n",
    "        metric = cache[str(min(a,b)) + \"-\" + str(max(a,b))][i]\n",
    "        summed_metric += metric\n",
    "    mean_metrics.append(summed_metric / (number_of_candidates-1))\n",
    "  selected_candidate_index = np.argmax(mean_metrics)\n",
    "  selected_candidate = all_predictions[selected_candidate_index][i]\n",
    "  representing_predictions.append(selected_candidate)\n",
    "\n",
    "with open(\"refs.txt\", 'w') as fp:\n",
    "  fp.write(\"\\n\".join(all_references))\n",
    "\n",
    "with open(\"sources.txt\", 'w') as fp:\n",
    "  fp.write(\"\\n\".join(all_sources))\n",
    "\n",
    "with open(target_experiment + \"-best.txt\", 'w') as fp:\n",
    "  fp.write(\"\\n\".join(best_predictions))\n",
    "\n",
    "with open(target_experiment + \"-worst.txt\", 'w') as fp:\n",
    "  fp.write(\"\\n\".join(worst_predictions))\n",
    "\n",
    "with open(target_experiment + \"-median.txt\", 'w') as fp:\n",
    "  fp.write(\"\\n\".join(median_predictions))\n",
    "\n",
    "with open(target_experiment + \"-most-similar.txt\", 'w') as fp:\n",
    "  fp.write(\"\\n\".join(representing_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
