{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYHyzkLIdUtA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#download the data\n",
    "!git clone https://dev:dtKN5sX9We7pw1soPB19@gitlab.lrz.de/josh-o/leichte-sprache-corpus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ-ABADcdegt"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#install dependencies (April 14, 2023)\n",
    "#pytorch 2.0.0+cu118\n",
    "#Python 3.9.16\n",
    "!pip install transformers==4.28.0 \n",
    "!pip install sentencepiece==0.1.98\n",
    "!pip install sacremoses==0.0.53\n",
    "!pip install evaluate==0.3.0\n",
    "!pip install sacrebleu==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b9OuN4idTfi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = 42\n",
    "set_seed(seed) # no direct effect on text generation\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:512\"\n",
    "\n",
    "PREFIX = \"../../leichte-sprache-corpus/aligned/20min/\"\n",
    "PREFIX_KURIER = \"../../leichte-sprache-corpus/aligned/kurier/\"\n",
    "PREFIX_AUGMENTED = \"../../leichte-sprache-corpus/aligned/20min/augmented/\"\n",
    "PREFIX_AUGMENTED_KURIER = \"../../leichte-sprache-corpus/aligned/kurier/augmented/\"\n",
    "\n",
    "mbart_path, mbart_revision = (\"facebook/mbart-large-cc25\", \"57cecec5a3185d3ec7b3021a53093cf96835a634\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DiSf3mUEMCQ"
   },
   "outputs": [],
   "source": [
    "#experiments\n",
    "\n",
    "decoder_path, custom_decoder_path, decoder_revision, augmentation, pretrained_weights, dropout_encoder, dropout_decoder, n_best = None, None, None, None, None, None, None, None\n",
    "\n",
    "#finetuned mBART decoder experiments\n",
    "\n",
    "#decoder_path, decoder_revision, data, experiment_name = (\"josh-oo/mbart-decoder-easy\", \"4d0d3be96da2a7327c8e38434c20a413b464b9a2\", \"Kurier\", \"ft_no_inputs\")\n",
    "#decoder_path, decoder_revision, data, experiment_name = (\"josh-oo/mbart-decoder-easy\", \"8c785939db9cf0851da9ea9003805bd78f7ec6cd\", \"Kurier\", \"ft_gaussian\")\n",
    "#decoder_path, decoder_revision, data, experiment_name = (\"josh-oo/mbart-decoder-easy\", \"0afac431259d36a796e9a08f428c70d537fd579d\", \"Kurier\", \"ft_bart_encodings\")\n",
    "\n",
    "#custom decoder experiments (using the custom gpt version)\n",
    "#custom_decoder_path, decoder_revision, data, experiment_name = (\"josh-oo/german-gpt2-easy\", \"f47cb790634678a65be9df09491edf154bd51f7c\", \"20min\", \"custom_gpt\")\n",
    "#custom_decoder_path, decoder_revision, data, pretrained_weights, experiment_name = (\"josh-oo/german-gpt2-easy\", \"f47cb790634678a65be9df09491edf154bd51f7c\", \"20min\", (\"josh-oo/calibrated-decoder\", \"28295adb39f2f8c82d981e53e519dd74959190cf\"), \"custom_pretrained_one2one\")\n",
    "#custom_decoder_path, decoder_revision, data, pretrained_weights, experiment_name = (\"josh-oo/german-gpt2-easy\", \"f47cb790634678a65be9df09491edf154bd51f7c\", \"20min\", (\"josh-oo/calibrated-decoder\", \"7bf004553a1f7d02e33dfece6b5901a20b873b69\"), \"custom_pretrained_bart_noise\")\n",
    "\n",
    "#dataset size experiments\n",
    "#n_best, data, experiment_name = (0.75, \"20min\", \"train_on_best_data_75p\")\n",
    "#n_best, data, experiment_name = (0.50, \"20min\", \"train_on_best_data_50p\")\n",
    "#n_best, data, experiment_name = (0.25, \"20min\", \"train_on_best_data_25p\")\n",
    "#n_best, data, experiment_name = (0.05, \"20min\", \"train_on_best_data_05p\")\n",
    "#n_best, data, experiment_name = (None, \"20min\", \"train_on_best_data_100p\")\n",
    "\n",
    "#data augmentation 20min\n",
    "#dropout_encoder, dropout_decoder, data, experiment_name = (0.0, 0.1, \"20min\", \"da_dropout_0_0\")\n",
    "#dropout_encoder, dropout_decoder, data, experiment_name = (0.1, 0.1, \"20min\", \"da_dropout_0_1\")\n",
    "#dropout_encoder, dropout_decoder, data, experiment_name = (0.3, 0.1, \"20min\", \"da_dropout_0_3\")\n",
    "#dropout_encoder, dropout_decoder, data, experiment_name = (0.3, 0.1, \"20min\", \"da_dropout_0_3\")\n",
    "\n",
    "#dropout_encoder, dropout_decoder, data, augmentation, experiment_name = (0.0, 0.1, \"20min\", \"inputs_simple_noise.csv\", \"da_simple_noise\")\n",
    "#dropout_encoder, dropout_decoder, data, augmentation, experiment_name = (0.0, 0.1, \"20min\", \"inputs_bart_noise.csv\",  \"da_bart_noise\")\n",
    "#dropout_encoder, dropout_decoder, data, augmentation, experiment_name = (0.0, 0.1, \"20min\", \"inputs_back_google.csv\", \"da_bt\")\n",
    "#dropout_encoder, dropout_decoder, data, augmentation, experiment_name = (0.0, 0.1, \"20min\", \"inputs_back_google_simple_noise.csv\", \"da_bt_noise\")\n",
    "#dropout_encoder, dropout_decoder, data, augmentation, experiment_name = (0.0, 0.1, \"20min\", \"inputs_english_deepl.csv\", \"da_english\")\n",
    "\n",
    "#data augmentation Kurier\n",
    "#TODO dropout_encoder, dropout_decoder, data, experiment_name = (0.0, 0.1, \"Kurier\", \"da_dropout_0_0\")\n",
    "#dropout_encoder, dropout_decoder, data, experiment_name = (0.1, 0.1, \"Kurier\", \"da_dropout_0_1\")\n",
    "#TODO dropout_encoder, dropout_decoder, data, experiment_name = (0.3, 0.1, \"Kurier\", \"da_dropout_0_3\")\n",
    "#TODO dropout_encoder, dropout_decoder, data, experiment_name = (0.3, 0.1, \"Kurier\", \"da_dropout_0_3\")\n",
    "\n",
    "dropout_encoder, dropout_decoder, data, augmentation, experiment_name = (0.0, 0.1, \"Kurier\", \"inputs_simple_noise.csv\", \"kurier_da_simple_noise\")\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "\n",
    "After defining all hyperparameters in the cell above, you can run all cells consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FItSNbMA8Aiz"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz0EIdNq_84S"
   },
   "source": [
    "## mBART with Custom GPT-2 decoder\n",
    "Ignore this section if you want to train a pure mBART model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYn29YUkAKZl"
   },
   "outputs": [],
   "source": [
    "#mBART with gpt-2 decoder:\n",
    "\n",
    "if custom_decoder_path is not None:\n",
    "    from transformers import EncoderDecoderModel, AutoModelForCausalLM, MBartModel, GPT2Tokenizer, MBartTokenizerFast\n",
    "\n",
    "    #prepare output_tokenizer\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "      outputs = token_ids_0 + [self.eos_token_id]\n",
    "      return outputs\n",
    "\n",
    "    GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "\n",
    "    output_tokenizer = GPT2Tokenizer.from_pretrained(custom_decoder_path)\n",
    "\n",
    "    output_tokenizer.pad_token_id = 1\n",
    "    output_tokenizer.bos_token_id = 0\n",
    "    output_tokenizer.eos_token_id = 2\n",
    "\n",
    "    input_tokenizer = MBartTokenizerFast.from_pretrained(mbart_path)\n",
    "    if hasattr(input_tokenizer, \"src_lang\"):\n",
    "      input_tokenizer.src_lang = \"de_DE\"\n",
    "\n",
    "    mbart = MBartModel.from_pretrained(mbart_path, revision=mbart_revision)\n",
    "    decoder = AutoModelForCausalLM.from_pretrained(custom_decoder_path, revision=decoder_revision)\n",
    "\n",
    "    model = EncoderDecoderModel(encoder=mbart.encoder,decoder=decoder)\n",
    "\n",
    "    # set decoding params\n",
    "    model.config.decoder_start_token_id = output_tokenizer.bos_token_id\n",
    "    model.config.eos_token_id = output_tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = 1\n",
    "    model.config.max_length = 1024\n",
    "\n",
    "    #freeze all\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    #make cross attention trainable\n",
    "    for module in model.decoder.transformer.h:\n",
    "      for param in module.crossattention.parameters():\n",
    "        param.requires_grad = True\n",
    "      for param in module.ln_cross_attn.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    if hasattr(model,'enc_to_dec_proj'):\n",
    "      model.enc_to_dec_proj.requires_grad = True\n",
    "\n",
    "    #unfreeze batchnorms\n",
    "    for module in model.modules():\n",
    "      if isinstance(module, torch.nn.LayerNorm):\n",
    "        for param in module.parameters():\n",
    "          param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHxB6S_zTkWC"
   },
   "source": [
    "### Load Pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNWoJPJ2TqdS"
   },
   "outputs": [],
   "source": [
    "if custom_decoder_path is not None and pretrained_weights is not None:\n",
    "  pretrained = EncoderDecoderModel.from_pretrained(pretrained_weights[0], revision=pretrained_weights[1])\n",
    "  pretrained.save_pretrained(\"temp\")\n",
    "\n",
    "  #update the original weights\n",
    "  all_states = model.state_dict()\n",
    "  update_states = torch.load(\"/temp/pytorch_model.bin\", map_location=device)\n",
    "  all_states.update(update_states)\n",
    "  model.load_state_dict(all_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujeKBBSK8CC-"
   },
   "source": [
    "## mBART\n",
    "Ignore this section if you want to train a model with custom decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dzSv6CZzD1K"
   },
   "outputs": [],
   "source": [
    "if custom_decoder_path is None:\n",
    "    from transformers import MBartForConditionalGeneration, MBartTokenizerFast, MBartConfig\n",
    "\n",
    "    model = MBartForConditionalGeneration.from_pretrained(mbart_path, revision=mbart_revision)\n",
    "\n",
    "    tokenizer = MBartTokenizerFast.from_pretrained(mbart_path, src_lang=\"de_DE\", tgt_lang=\"de_DE\")\n",
    "    input_tokenizer, output_tokenizer = tokenizer, tokenizer\n",
    "\n",
    "    # set decoding params\n",
    "    model.config.decoder_start_token_id=250003\n",
    "    model.config.forced_bos_token_id=0\n",
    "    model.config.max_length = 1024\n",
    "\n",
    "    #freeze all\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    #make cross attention trainable\n",
    "    for layer in model.model.decoder.layers:\n",
    "      for param in layer.encoder_attn.parameters():\n",
    "        param.requires_grad = True\n",
    "      for param in layer.encoder_attn_layer_norm.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    #unfreeze batchnorms\n",
    "    for module in model.modules():\n",
    "      if isinstance(module, torch.nn.LayerNorm):\n",
    "        for param in module.parameters():\n",
    "          param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu3tDxfi7wpF"
   },
   "source": [
    "### Load Custom mMBART Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eY04-_5v7wAH"
   },
   "outputs": [],
   "source": [
    "if custom_decoder_path is None and decoder_path is not None and decoder_revision is not None:\n",
    "  #load the finetuned mBART decoders weights\n",
    "  from transformers import MBartForCausalLM\n",
    "\n",
    "  decoder = MBartForCausalLM.from_pretrained(decoder_path, revision=decoder_revision)\n",
    "  decoder.save_pretrained(\"temp\")\n",
    "\n",
    "  #update the decoder weights of the full mBART\n",
    "  all_states = model.state_dict()\n",
    "  update_states = torch.load(\"/content/temp/pytorch_model.bin\", map_location=device)\n",
    "  all_states.update(update_states)\n",
    "  model.load_state_dict(all_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuHaMFctDRUx"
   },
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DdeEKMKDVD0"
   },
   "outputs": [],
   "source": [
    "def set_dropout(model, p_encoder, p_decoder):\n",
    "  \n",
    "  model.get_encoder().dropout = p_encoder\n",
    "  for layer in model.get_encoder().layers:\n",
    "    layer.dropout = p_encoder\n",
    "\n",
    "  model.get_decoder().dropout = p_decoder\n",
    "  for layer in model.get_decoder().layers:\n",
    "    layer.dropout = p_decoder\n",
    "\n",
    "if dropout_encoder is not None and dropout_decoder is not None:\n",
    "  set_dropout(model, p_encoder=dropout_encoder, p_decoder=dropout_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge8cnsnr1Mqg"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SamgZZkQApBy"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,concatenate_datasets, Features, Value\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalize(text):\n",
    "  text['normal_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['normal_phrase'].strip())\n",
    "  text['simple_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['simple_phrase'].strip())\n",
    "  return text\n",
    "\n",
    "def tokenize(text, input_tokenizer, output_tokenizer, max_input_length):\n",
    "  inputs = input_tokenizer(text[\"normal_phrase\"], return_tensors=\"np\")\n",
    "  labels = output_tokenizer(text[\"simple_phrase\"], return_tensors=\"np\", truncation=True, max_length=max_input_length)\n",
    "  inputs['labels'] = labels['input_ids']\n",
    "  return inputs\n",
    "\n",
    "def count(text):\n",
    "  #calculate the length token which is used to group the data samples\n",
    "  #we use len(input) * len(output) as it models the maximum GPU memory consumption the best\n",
    "  #(we want to have the data sample with the highest memory consumption at the first place to force early Out-Of-Memory issues)\n",
    "  text['length'] = len(text['input_ids']) * len(text['labels'])\n",
    "  return text\n",
    "\n",
    "def get_dataset(data_files, input_tokenizer, output_tokenizer, name=None, augmentation_file=None,max_input_length=None, augmentation_tokenizer=None, seed=None, n_best=None):\n",
    "  features = Features({'normal_phrase': Value('string'), 'simple_phrase': Value('string')})\n",
    "  data = load_dataset(\"csv\",name=name, data_files=data_files, features=features)\n",
    "    \n",
    "  if n_best is not None:\n",
    "    print(\"\\nTest\\n\")\n",
    "    #select only the n best train samples according to a precomputed loss\n",
    "    df = pd.read_csv(data_files['train'])\n",
    "    data['train'] = data['train'].add_column(\"loss\",df['loss'])\n",
    "    \n",
    "    #calculate eachs samples deviation from the mean loss to exclude outliers that are either too hard or too easy according to their loss\n",
    "    df['deviation'] = (df['loss'] - df['loss'].mean()).abs()\n",
    "    n = int(len(df.index) * n_best)\n",
    "    df = df.nsmallest(n, 'deviation', keep='first')\n",
    "    min_pre_loss = df['loss'].min()\n",
    "    max_pre_loss = df['loss'].max()\n",
    "    \n",
    "    data['train'] = data['train'].filter(lambda example: example[\"loss\"] < max_pre_loss and example[\"loss\"] > min_pre_loss)\n",
    "    data['train'] = data['train'].remove_columns(['loss'])\n",
    "    \n",
    "  data = data.map(normalize, num_proc=4)\n",
    "  data = data.map(lambda rows: tokenize(rows, input_tokenizer, output_tokenizer, max_input_length), batched=True)\n",
    "  if \"train\" in data:\n",
    "    data['train'] = data['train'].map(count, num_proc=4)\n",
    "    data = data.remove_columns([column for column in data.column_names['train'] if column not in ['labels','input_ids','attention_mask','length']])\n",
    "  else:\n",
    "    data = data.remove_columns([column for column in data.column_names['test'] if column not in ['labels','input_ids','attention_mask','length']])\n",
    "\n",
    "  if augmentation_file is not None:\n",
    "    if augmentation_tokenizer is None:\n",
    "      augmentation_tokenizer = input_tokenizer\n",
    "    #add augmented input to the train dataset if given\n",
    "    data_a = load_dataset(\"csv\", data_files=augmentation_file, features=features)\n",
    "    data_a = data_a.map(normalize, num_proc=4)\n",
    "    data_a = data_a.map(lambda row: augmentation_tokenizer(row[\"normal_phrase\"], return_tensors=\"np\", truncation=True, max_length=max_input_length), batched=True)\n",
    "    data_a = data_a.rename_column(\"input_ids\", \"augmented_ids\")\n",
    "    data_a = data_a.rename_column(\"attention_mask\", \"augmented_mask\")\n",
    "    data['train'] = concatenate_datasets([data['train'], data_a['train']], axis=1)\n",
    "    data['train'] = data['train'].remove_columns([column for column in data.column_names['train'] if column not in ['labels','input_ids','attention_mask','length','augmented_ids','augmented_mask']])\n",
    "\n",
    "    #we want to randomize the order of original and augmented inputs to avoid the case of first train all original and then all augmented version\n",
    "    #add an random permutation of indicators, indicating whether augmented inputs should be used in even or odd epochs\n",
    "    #use a seed to keep it reproducable\n",
    "    data_length = len(data['train'])\n",
    "    indicators = np.ones(data_length, dtype='bool')\n",
    "    indicators[:data_length//2] = False\n",
    "    np.random.seed(seed)\n",
    "    indicators = np.random.permutation(indicators).tolist()\n",
    "    indicators\n",
    "\n",
    "    data['train'] = data['train'].add_column(\"indicator\",indicators)\n",
    "\n",
    "  if max_input_length is not None:\n",
    "    data = data.filter(lambda example: len(example[\"input_ids\"]) < max_input_length)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2TgMeY2YuvE"
   },
   "outputs": [],
   "source": [
    "if data == \"20min\":\n",
    "  #train on 20min\n",
    "  augmentation_tokenizer = None\n",
    "  if augmentation:\n",
    "    augmentation = PREFIX_AUGMENTED + augmentation\n",
    "    if \"english\" in augmentation:\n",
    "      augmentation_tokenizer = None #TODO load the corresponding tokenizer\n",
    "\n",
    "  data_files_20_min = {'train': PREFIX + \"20min_aligned_train.csv\", 'val': PREFIX + \"20min_aligned_dev.csv\", 'test': PREFIX + \"20min_aligned_test.csv\"}\n",
    "  data_files_kurier = {'val': PREFIX_KURIER + \"kurier_aligned_dev.csv\", 'test': PREFIX_KURIER + \"kurier_aligned_test.csv\"}\n",
    "\n",
    "  train_name = \"20min\"\n",
    "  test_name = \"KURIER\"\n",
    "  steps_to_train = 2000\n",
    "\n",
    "  data_train = get_dataset(data_files_20_min, input_tokenizer, output_tokenizer, train_name, augmentation, augmentation_tokenizer=augmentation_tokenizer, max_input_length=model.config.max_length,seed=seed, n_best=n_best)\n",
    "  data_test  = get_dataset(data_files_kurier, input_tokenizer, output_tokenizer, test_name, max_input_length=model.config.max_length,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gNXfVbfuq5Y"
   },
   "outputs": [],
   "source": [
    "if data == \"Kurier\":\n",
    "  #train on kurier\n",
    "\n",
    "  augmentation_tokenizer = None\n",
    "  if augmentation:\n",
    "    augmentation = PREFIX_AUGMENTED_KURIER + augmentation\n",
    "    if \"english\" in augmentation:\n",
    "      augmentation_tokenizer = None #TODO load the corresponding tokenizer\n",
    "    \n",
    "  data_files_kurier = {'train': PREFIX_KURIER + \"kurier_aligned_train.csv\",'val': PREFIX_KURIER + \"kurier_aligned_dev.csv\", 'test': PREFIX_KURIER + \"kurier_aligned_test.csv\"}\n",
    "  data_files_20_min = {'val': PREFIX + \"20min_aligned_dev.csv\", 'test': PREFIX + \"20min_aligned_test.csv\"}\n",
    "\n",
    "  train_name = \"KURIER\"\n",
    "  test_name = \"20min\"\n",
    "  steps_to_train = 1000\n",
    "\n",
    "  data_train = get_dataset(data_files_kurier, input_tokenizer, input_tokenizer, train_name,augmentation, augmentation_tokenizer=augmentation_tokenizer, max_input_length=model.config.max_length,seed=seed, n_best=n_best)\n",
    "  data_test  = get_dataset(data_files_20_min, input_tokenizer, input_tokenizer, test_name,  max_input_length=model.config.max_length,seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cunWz59XBMwu"
   },
   "source": [
    "# Trainingstuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5PLMf3NAYEq"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irV3OmiphasG"
   },
   "outputs": [],
   "source": [
    "class CustomCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "\n",
    "        augmented_ids = [feature[\"augmented_ids\"] for feature in features] if \"augmented_ids\" in features[0].keys() else None\n",
    "        augmented_mask = [feature[\"augmented_mask\"] for feature in features] if \"augmented_mask\" in features[0].keys() else None\n",
    "\n",
    "        if augmented_ids is not None and augmented_mask is not None:\n",
    "            #process augmentation data\n",
    "            temp_features = []\n",
    "            for feature in features:\n",
    "                temp_feature = {}\n",
    "                temp_feature['input_ids'] = feature.pop(\"augmented_ids\")\n",
    "                temp_feature['attention_mask'] = feature.pop(\"augmented_mask\")\n",
    "                temp_features.append(temp_feature)\n",
    "\n",
    "            temp_features = self.__call__(temp_features)\n",
    "            for i, feature in enumerate(features):\n",
    "                feature[\"augmented_ids\"] = temp_features[\"input_ids\"][i].tolist()\n",
    "                feature[\"augmented_mask\"] = temp_features[\"attention_mask\"][i].tolist()\n",
    "\n",
    "        return super().__call__(features, return_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kffOERl4IlAZ"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "class AugmentationTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        self.current_epoch = 0\n",
    "        self.even_epoch = torch.tensor(True, device=model.device)\n",
    "      \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def evaluate(self,eval_dataset = None,ignore_keys = None,metric_key_prefix = \"eval\",**gen_kwargs):\n",
    "        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix, num_beams=1, do_sample=False)#, top_k=3, penalty_alpha=0.6)\n",
    "\n",
    "    def predict(self,test_dataset,ignore_keys = None,metric_key_prefix = \"test\",**gen_kwargs):\n",
    "        return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix, num_beams=3, do_sample=False)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        if self.state.epoch > self.current_epoch:\n",
    "          self.current_epoch = self.state.epoch\n",
    "          self.even_epoch = ~self.even_epoch\n",
    "\n",
    "        length = inputs.pop('length', None)\n",
    "        indicator = inputs.pop('indicator', None)\n",
    "        augmented_ids = inputs.pop(\"augmented_ids\", None)\n",
    "        augmented_mask = inputs.pop(\"augmented_mask\", None)\n",
    "\n",
    "        original_ids = inputs.pop(\"input_ids\")\n",
    "        original_mask = inputs.pop(\"attention_mask\")\n",
    "\n",
    "        #TODO handle different paddings for batchsizes > 1\n",
    "        #inputs['input_ids'] = torch.where(indicator == self.even_epoch, original_ids, augmented_ids)\n",
    "        #inputs['attention_mask'] = torch.where(indicator == self.even_epoch, original_mask, augmented_mask)\n",
    "\n",
    "        if augmented_ids is not None and indicator is not None and (indicator == self.even_epoch).all():\n",
    "          inputs['input_ids'] = augmented_ids\n",
    "          inputs['attention_mask'] = augmented_mask\n",
    "        else:\n",
    "          inputs['input_ids'] = original_ids\n",
    "          inputs['attention_mask'] = original_mask\n",
    "\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2Dm5w7FhWup"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def compute_translation_metrics(input_tokenizer, output_tokenizer, pred, control_tokens):\n",
    "\n",
    "    input_ids = pred.inputs\n",
    "    label_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    input_ids[input_ids == -100] = input_tokenizer.pad_token_id\n",
    "    label_ids[label_ids == -100] = output_tokenizer.pad_token_id\n",
    "    pred_ids[pred_ids == -100] = output_tokenizer.pad_token_id\n",
    "\n",
    "    input_str_list = input_tokenizer.batch_decode(input_ids, skip_special_tokens=True,\n",
    "                                                  clean_up_tokenization_spaces=False)\n",
    "    pred_str_list = output_tokenizer.batch_decode(pred_ids, skip_special_tokens=True,\n",
    "                                                  clean_up_tokenization_spaces=False)\n",
    "    label_str_list = output_tokenizer.batch_decode(label_ids, skip_special_tokens=True,\n",
    "                                                   clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    if control_tokens == True:\n",
    "      for i in range(0, len(input_str_list)):\n",
    "        input_str_list[i] = input_str_list[i].split(' ', 1)[1]\n",
    "\n",
    "    label_str_list = [[label] for label in label_str_list]\n",
    "\n",
    "    sari = evaluate.load(\"sari\")\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "    sari_score = sari.compute(sources=input_str_list, predictions=pred_str_list, references=label_str_list)\n",
    "    bleu_score = bleu.compute(predictions=pred_str_list, references=label_str_list)\n",
    "\n",
    "    translation_result = {\n",
    "        'sari':sari_score['sari'],\n",
    "        'bleu':bleu_score['bleu']*100\n",
    "    }\n",
    "\n",
    "    return {key: sum(value) / len(value) if isinstance(value, Iterable) else value for (key, value) in\n",
    "            translation_result.items()}\n",
    "\n",
    "compute_metrics = lambda pred: compute_translation_metrics(input_tokenizer, output_tokenizer, pred, control_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfHd8eIS7J8N"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXsEA3Up7LUn"
   },
   "outputs": [],
   "source": [
    "data_collator = CustomCollator(tokenizer=input_tokenizer, model=model, pad_to_multiple_of=8)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    include_inputs_for_metrics=True,\n",
    "    generation_max_length=1024,\n",
    "    \n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio=0.05,\n",
    "    #warmup_steps=100,\n",
    "    #max_steps=steps_to_train,\n",
    "    output_dir=\"../results\",\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='no',\n",
    "    learning_rate=3e-5, \n",
    "    weight_decay=0.01, \n",
    "    per_device_eval_batch_size=4, \n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    #logging_steps= steps_to_train // 4,\n",
    "    group_by_length=True,\n",
    "    seed=seed,\n",
    "    data_seed=seed,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    optim='adamw_torch',\n",
    ")\n",
    "\n",
    "trainer = AugmentationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics = compute_metrics,\n",
    "    train_dataset=data_train['train'],\n",
    "    eval_dataset=data_train['val'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfZeKO263lgO"
   },
   "source": [
    "# Upload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiKCB-T5TUrf"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acEFNXyBiBO6"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"mbart-ts\", commit_message=\"Trained on Kurier [Simple Noise Decoder-Dropout 0.1] (2 epochs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFs4phjiYfHx"
   },
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_dataset=data_train['test'])\n",
    "print(preds.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHp2PRDx6Uxy"
   },
   "source": [
    "## Auto Disconnect from Colab to Save Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Scx6BPJ-Yoxz"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Iz0EIdNq_84S",
    "ujeKBBSK8CC-",
    "PuHaMFctDRUx",
    "ge8cnsnr1Mqg"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
