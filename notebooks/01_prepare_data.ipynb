{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CriNJc7dem5y"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#download the data\n",
    "!git clone https://dev:dtKN5sX9We7pw1soPB19@gitlab.lrz.de/josh-o/leichte-sprache-corpus.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "\n",
    "This notebook is intended to only execute the section, you currently need for data preparation!  \n",
    "Running cells from different sections may cause unintended behavior.  \n",
    "However, the cell above this cell is for general purpose and should be executed in any case if your are on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7N3BKQD5EPp"
   },
   "source": [
    "# Compute Controll Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoPuvLkSVYYX"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#install dependencies (April 14, 2023)\n",
    "#pytorch 2.0.0+cu118\n",
    "#Python 3.9.16\n",
    "!pip install transformers==4.28.0 \n",
    "!pip install sentencepiece==0.1.98\n",
    "!pip install sacremoses==0.0.53\n",
    "!pip install levenshtein==0.20.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6gLNjLaAN6G"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8VdHxUITUdp"
   },
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.de.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSByXfpgWy9F"
   },
   "outputs": [],
   "source": [
    "#compute char ratio\n",
    "def char_ratio(row):\n",
    "  return len(row['simple_phrase']) / len(row['normal_phrase'])\n",
    "\n",
    "dataframe['nbchars'] = dataframe.apply(char_ratio, axis=1, result_type='reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0z2W6XQWMry"
   },
   "outputs": [],
   "source": [
    "#compute Levenshtein similarity\n",
    "\n",
    "import Levenshtein as lev\n",
    "\n",
    "def lev_ratio(row):\n",
    "  return lev.ratio(row['normal_phrase'], row['simple_phrase'])\n",
    "\n",
    "dataframe['lev_sim'] = dataframe.apply(lev_ratio, axis=1, result_type='reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lninfboS-UY0"
   },
   "outputs": [],
   "source": [
    "#compute mean sentence depth\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def get_dependency_tree_depth(sentence):\n",
    "  def get_subtree_depth(node):\n",
    "      if len(list(node.children)) == 0:\n",
    "          return 0\n",
    "      return 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "\n",
    "  tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in nlp(sentence).sents]\n",
    "\n",
    "  if len(tree_depths) == 0:\n",
    "      return 0\n",
    "  return np.mean(tree_depths)\n",
    "\n",
    "def dep_ratio(row):\n",
    "  return get_dependency_tree_depth(row['normal_phrase']) / get_dependency_tree_depth(row['simple_phrase'])\n",
    "\n",
    "dataframe['dep'] = dataframe.apply(dep_ratio, axis=1, result_type='reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERc_N-QKPUB-"
   },
   "outputs": [],
   "source": [
    "#compute complexity\n",
    "\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('german'))\n",
    "\n",
    "#FASTTEXT_EMBEDDINGS_PATH = \"/content/cc.de.300.vec.gz\"\n",
    "FASTTEXT_EMBEDDINGS_PATH = \"/content/wiki.de.vec\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in to_words(text) if w.lower() not in stopwords])\n",
    "\n",
    "def to_words(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "def remove_punctuation_characters(text):\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "def remove_punctuation_characters(text):\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def is_punctuation(word):\n",
    "    return remove_punctuation_characters(word) == ''\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def remove_punctuation_tokens(text):\n",
    "    return ' '.join([w for w in to_words(text) if not is_punctuation(w)])\n",
    "\n",
    "def count_lines(filepath):\n",
    "    n_lines = 0\n",
    "    with Path(filepath).open() as f:\n",
    "        for l in f:\n",
    "            n_lines += 1\n",
    "    return \n",
    "\n",
    "def yield_lines(filepath, n_lines=float('inf'), prop=1):\n",
    "    if prop < 1:\n",
    "        assert n_lines == float('inf')\n",
    "        n_lines = int(prop * count_lines(filepath))\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if i >= n_lines:\n",
    "                break\n",
    "            yield l.rstrip('\\n')\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_word2rank(vocab_size=np.inf):\n",
    "    # TODO: Decrease vocab size or load from smaller file\n",
    "    word2rank = {}\n",
    "    line_generator = yield_lines(FASTTEXT_EMBEDDINGS_PATH)\n",
    "    next(line_generator)  # Skip the first line (header)\n",
    "    for i, line in enumerate(line_generator):\n",
    "        if (i + 1) > vocab_size:\n",
    "            break\n",
    "        word = line.split(' ')[0]\n",
    "        word2rank[word] = i\n",
    "    return word2rank\n",
    "\n",
    "\n",
    "def get_rank(word):\n",
    "    return get_word2rank().get(word, len(get_word2rank()))\n",
    "\n",
    "def get_log_rank(word):\n",
    "    return np.log(1 + get_rank(word))\n",
    "\n",
    "def get_lexical_complexity_score(sentence):\n",
    "    words = to_words(remove_stopwords(remove_punctuation_tokens(sentence)))\n",
    "    words = [word for word in words if word in get_word2rank()]\n",
    "    if len(words) == 0:\n",
    "        return np.log(1 + len(get_word2rank()))  # TODO: This is completely arbitrary\n",
    "    return np.quantile([get_log_rank(word) for word in words], 0.75)\n",
    "\n",
    "\n",
    "def complexity_ratio(row):\n",
    "  return  get_lexical_complexity_score(row['simple_phrase']) / get_lexical_complexity_score(row['normal_phrase'])\n",
    "\n",
    "dataframe['rank'] = dataframe.apply(complexity_ratio, axis=1, result_type='reduce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iY9vaPrizsIK"
   },
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"valid_tokens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToQwbOHsGlpI"
   },
   "outputs": [],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDFE4-5S0INM"
   },
   "source": [
    "# Prepare mlsum dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w-Zr8nWOnCy"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rhk6drmW0L3Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mlsum\", \"de\")\n",
    "\n",
    "articles = dataset['train']['text'][:17100]\n",
    "data = {'normal_phrase' : articles, 'simple_phrase' : articles}\n",
    "\n",
    "regular_df = pd.DataFrame(data)\n",
    "regular_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLQIl9l3xEf2"
   },
   "source": [
    "# Prepare DeepL files\n",
    "As deepL requires .txt files with a maximum of 1 million characters, we split the csv fileinto .txt files of 1 million characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsgWAuhcxMhS"
   },
   "outputs": [],
   "source": [
    "!mkdir full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrMe7x9AxJle"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/20min/20min_aligned_train.csv\")\n",
    "#dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/20min/augmented/pure_simple_english_deepl.csv\")\n",
    "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/kurier_aligned_train.csv\")\n",
    "\n",
    "full_text = list(dataframe['normal_phrase'].values)\n",
    "\n",
    "i = 0\n",
    "while len(full_text) > 0:\n",
    "  total_length = 0\n",
    "  current_items = []\n",
    "  while len(full_text) > 0 and (total_length + len(full_text[0]) + 5 < 1_000_000):\n",
    "    current_item = full_text.pop(0)\n",
    "    current_items.append(current_item)\n",
    "    total_length += len(current_item) + 5\n",
    "  \n",
    "  full_text_part = \"\\n<#>\\n\".join(current_items)\n",
    "  print(f\"Part {i} has {total_length} chars\")\n",
    "  with open(f'full_text/part_{str(i).zfill(2)}.txt', 'w') as f:\n",
    "    f.write(full_text_part)\n",
    "  i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_ooeR5UxPPQ"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/full_text.zip /content/full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otJunvhAEjDE"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install evaluate\n",
    "!pip install sacremoses\n",
    "!pip install sacrebleu==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVlalFXi3CCn"
   },
   "source": [
    "## DeepL tp CSV\n",
    "Finally the .txt files are parsed back into the original .csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVooxIH5IwMG"
   },
   "outputs": [],
   "source": [
    "full_text = \"\"\n",
    "\n",
    "number_of_files = 10\n",
    "\n",
    "PDF_PREFIX = \"/content/\"\n",
    "\n",
    "parts = [str(i).zfill(2) for i in range(0, number_of_files)]\n",
    "for number in parts:\n",
    "  print(f\"Process {number}\")\n",
    "  file_path = (PDF_PREFIX + f'part_{number} de.txt')\n",
    "  file_str = open(file_path, 'r').read()\n",
    "  full_text += file_str.replace(\"\\n\",\"\")\n",
    "  full_text += \"<#>\"\n",
    "\n",
    "en_texts = full_text.split(\"<#>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iiof61jV5L2t"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/kurier_aligned_train.csv\")\n",
    "\n",
    "with open('inputs_back_deepl.csv', 'w') as myfile:\n",
    "  myfile.write('normal_phrase,simple_phrase\\n')\n",
    "for i, en_text in enumerate(en_texts):\n",
    "  if en_text == \"\" and i == len(en_texts) -1:\n",
    "    continue\n",
    "  with open('inputs_back_deepl.csv', 'a') as myfile:\n",
    "    csvwriter = csv.writer(myfile)\n",
    "    normal_sample = en_text.strip()\n",
    "    simple_sample = dataframe.iloc[i]['simple_phrase']\n",
    "    csvwriter.writerow([normal_sample,simple_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khIQhlmnScNg"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"inputs_back_deepl.csv\")\n",
    "dataframe.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OifSDvwDRy58"
   },
   "source": [
    "# Prepare Google Translate Files\n",
    "Same thing as for deepL, but Google allows excel files which is more convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogfEV8p_0Z21"
   },
   "outputs": [],
   "source": [
    "!mkdir full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A04xA__sR19_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#to excel\n",
    "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/augmented/inputs_english_deepl.csv\")\n",
    "cropped_df = dataframe.copy()['normal_phrase']\n",
    "\n",
    "splits = 1\n",
    "\n",
    "part_length = int(len(cropped_df.index)/splits)\n",
    "\n",
    "for i in range(0, splits):\n",
    "  with pd.ExcelWriter(f'full_text/output_{str(i).zfill(2)}.xlsx') as writer:  \n",
    "    cropped_df[i*part_length:(i+1)*part_length].to_excel(writer, sheet_name='Sheet_1')\n",
    "\n",
    "if part_length*splits < len(cropped_df.index):\n",
    "  with pd.ExcelWriter(f'full_text/output_{str(splits).zfill(2)}.xlsx') as writer:  \n",
    "    cropped_df[splits*part_length:].to_excel(writer, sheet_name='Sheet_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRnp0q2ie-eY"
   },
   "source": [
    "## Google -> CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKfToP6GfBaN"
   },
   "outputs": [],
   "source": [
    "from pandas.io.formats.format import DataFrameFormatter\n",
    "full_text = \"\"\n",
    "\n",
    "number_of_files = 1\n",
    "\n",
    "PDF_PREFIX = \"/content/\"\n",
    "\n",
    "all_dataframes = []\n",
    "\n",
    "parts = [str(i).zfill(2) for i in range(0, number_of_files)]\n",
    "for number in parts:\n",
    "  print(f\"Process {number}\")\n",
    "  file_path = (PDF_PREFIX + f'output_{number}_de.xlsx')\n",
    "  current_dataframe = pd.read_excel(file_path, index_col=0)  \n",
    "  all_dataframes.append(current_dataframe)\n",
    "\n",
    "back_translated_df = pd.concat(all_dataframes)\n",
    "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/kurier_aligned_train.csv\")\n",
    "dataframe['normal_phrase'] = back_translated_df['normale_phrase']\n",
    "dataframe.to_csv('pure_simple_back_google.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvZI8HcOkao7"
   },
   "source": [
    "# Pure Simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8n0I324B7Sf"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/20min/20min_aligned_train.csv\")\n",
    "dataframe['normal_phrase'] = dataframe['simple_phrase']\n",
    "dataframe.to_csv(\"pure_simple.csv\", index=False)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXLiczpOfNAJ"
   },
   "source": [
    "# Add Simple Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_fYbNaKfSDa"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/valentinmace/noisy-text.git noisy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrrWPAwBhWZM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/noisy_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33IgJPhgkqDa"
   },
   "outputs": [],
   "source": [
    "from noise_functions import delete_random_token, replace_random_token, random_token_permutation\n",
    "\n",
    "def add_noise(line):\n",
    "  line = delete_random_token(line, probability=0.1)\n",
    "  line = replace_random_token(line, probability=0.1, filler_token=\"<mask>\")\n",
    "  line = random_token_permutation(line, _range=3)\n",
    "  return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHvnBYoglHlB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/kurier_aligned_test.csv\")\n",
    "dataframe['normal_phrase'] = dataframe['normal_phrase'].apply(add_noise)\n",
    "dataframe.to_csv(\"../simple_noise.csv\", index=False)\n",
    "dataframe.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB_QOJk2Ib7k"
   },
   "source": [
    "# Add Bart Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LRePSuWXtNk"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pv2lgf_fYTyN"
   },
   "outputs": [],
   "source": [
    "from transformers import MBartTokenizer\n",
    "\n",
    "tokenizer =  MBartTokenizer.from_pretrained(\"josh-oo/modified-mbart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOshkuJoJsqc"
   },
   "outputs": [],
   "source": [
    "#adapted from https://github.com/facebookresearch/fairseq/blob/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/data/denoising_dataset.py#L257\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from random import randrange\n",
    "\n",
    "def poison_distribution(poisson_lambda=3):\n",
    "  _lambda = poisson_lambda\n",
    "\n",
    "  lambda_to_the_k = 1\n",
    "  e_to_the_minus_lambda = math.exp(-_lambda)\n",
    "  k_factorial = 1\n",
    "  ps = []\n",
    "  for k in range(0, 128):\n",
    "      ps.append(e_to_the_minus_lambda * lambda_to_the_k / k_factorial)\n",
    "      lambda_to_the_k *= _lambda\n",
    "      k_factorial *= k + 1\n",
    "      if ps[-1] < 0.0000001:\n",
    "          break\n",
    "  ps = torch.FloatTensor(ps)\n",
    "  return torch.distributions.Categorical(ps)\n",
    "\n",
    "def split_into_sentences(text):\n",
    "  # Zerlegung des Textes in Sätze nach diesen Regeln:\n",
    "  # (?<!\\.\\.)\\s -> Keine Trennung bei ...\n",
    "  # (?<!\\w\\.\\w.)\\s \t\t-> Keine Trennung wenn zwei Zeichen mit einem Punkt in der Mitte und am Ende dem Leerzeichen vorausgehen (z.B. etc.)\n",
    "  # (?<![0-9]\\.)\\s \t\t-> Keine Trennung wenn eine Nummer folgend von einem Punkt dem Leerzeichen vorausgeht (9. etc.)\n",
    "  # (?<![0-9][0-9]\\.)\\s \t-> Keine Trennung wenn zwei Nummern folgend von einem Punkt dem Leerzeichen vorausgehen (18. etc.)\n",
    "  # (?<![A-Z]\\.)\t\t\t-> Keine Trennung wenn ein Großbuchstabe gefolgt von einem Punkt dem Leerzeichen vorausgehen (W. etc)\n",
    "  # (?<![A-Z][a-z]\\.)\\s \t-> Keine Trennung wenn ein Großbuchstabe gefolgt von einem Kleibuchstaben und einem Punkt dem Leerzeichen vorausgehen (Dr. etc)\n",
    "  # (?<=\\.|\\?|\\!)\\s \t\t-> Trennen wenn ein Punkt, Fragezeichen oder Ausrufezeichen dem Leerzeichen vorausgehen\n",
    "  \n",
    "  sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![0-9]\\.)(?<![0-9][0-9]\\.)(?<![A-Z]\\.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", text)\n",
    "  sentences = [s for s in sentences if s]\n",
    "  return sentences\n",
    "\n",
    "\n",
    "def permute_sentences(line, p=1.0):\n",
    "  sentences = split_into_sentences(line)\n",
    "\n",
    "  result = sentences.copy()\n",
    "\n",
    "  num_sentences = len(sentences)\n",
    "  num_to_permute = math.ceil((num_sentences * 2 * p) / 2.0)\n",
    "  substitutions = torch.randperm(num_sentences)[:num_to_permute]\n",
    "  ordering = torch.arange(0, num_sentences)\n",
    "  ordering[substitutions] = substitutions[torch.randperm(num_to_permute)]\n",
    "\n",
    "  for result_index, i in enumerate(ordering):\n",
    "    result[result_index] = sentences[i]\n",
    "  return \" \".join(result)\n",
    "\n",
    "def get_random_token():\n",
    "  while True:\n",
    "    index = randrange(tokenizer.vocab_size)\n",
    "    out = tokenizer.decode([index])\n",
    "    if len(tokenizer([out])['input_ids'][0]) == 3: #TODO adapt for other tokenizers\n",
    "      return out\n",
    "\n",
    "def add_whole_word_mask(line, p=0.3, random_ratio=0.1):\n",
    "  words = line.split(\" \")\n",
    "  num_to_mask = int(math.ceil(len(words) * p))\n",
    "  num_inserts = 0\n",
    "  if num_to_mask == 0:\n",
    "      return line\n",
    "\n",
    "  mask_span_distribution = poison_distribution(poisson_lambda=1.8)\n",
    "  lengths = mask_span_distribution.sample(sample_shape=(num_to_mask,))\n",
    "  # Make sure we have enough to mask\n",
    "  cum_length = torch.cumsum(lengths, 0)\n",
    "  while cum_length[-1] < num_to_mask:\n",
    "      lengths = torch.cat(\n",
    "          [\n",
    "              lengths,\n",
    "              mask_span_distribution.sample(sample_shape=(num_to_mask,)),\n",
    "          ],\n",
    "          dim=0,\n",
    "      )\n",
    "      cum_length = torch.cumsum(lengths, 0)\n",
    "\n",
    "  # Trim to masking budget\n",
    "  i = 0\n",
    "  while cum_length[i] < num_to_mask:\n",
    "      i += 1\n",
    "  lengths[i] = num_to_mask - (0 if i == 0 else cum_length[i - 1])\n",
    "  num_to_mask = i + 1\n",
    "  lengths = lengths[:num_to_mask]\n",
    "\n",
    "  # Handle 0-length mask (inserts) separately\n",
    "  lengths = lengths[lengths > 0]\n",
    "  num_inserts = num_to_mask - lengths.size(0)\n",
    "  num_to_mask -= num_inserts\n",
    "  if num_to_mask == 0:\n",
    "      return \" \".join(words)\n",
    "\n",
    "  assert (lengths > 0).all()\n",
    "  indices = torch.randperm(len(words))[:num_to_mask]\n",
    "  mask_random = torch.FloatTensor(num_to_mask).uniform_() < random_ratio\n",
    "  source_length = len(words)\n",
    "\n",
    "  #to_keep = torch.ones(source_length, dtype=torch.bool)\n",
    "  # keep index, but replace it with [MASK]\n",
    "  index_to_remove = []\n",
    "  to_keep = torch.ones(len(words), dtype=torch.bool)\n",
    "  for i, index in enumerate(indices):\n",
    "    words[index] = \"<mask>\"\n",
    "    if mask_random[i]:\n",
    "      words[index] = get_random_token()\n",
    "    current_length = lengths[i]\n",
    "    for shift in range(1,current_length):\n",
    "      if index + shift < len(to_keep):\n",
    "        to_keep[index + shift] = False\n",
    "\n",
    "  assert len(lengths.size()) == 1\n",
    "  assert lengths.size() == indices.size()\n",
    "  \n",
    "  words = np.array(words)[to_keep]\n",
    "\n",
    "  return \" \".join(words)\n",
    "\n",
    "def crop_texts(line, max_length=512):\n",
    "  tokens = tokenizer.tokenize(line)[:max_length]\n",
    "  return tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "def add_bart_noise(line, permutation=1.0, masking=0.3, random_masking=0.1):\n",
    "  if permutation > 0:\n",
    "    line = permute_sentences(line, permutation)\n",
    "  if masking > 0:\n",
    "    line = add_whole_word_mask(line, masking, random_masking)\n",
    "  return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cDFCTD4BCf6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/20min/20min_aligned_train.csv\")\n",
    "#dataframe['normal_phrase'] = dataframe['normal_phrase'].apply(crop_texts)\n",
    "dataframe['normal_phrase'] = dataframe['normal_phrase'].apply(add_bart_noise)\n",
    "dataframe.to_csv(\"bart_noise.csv\", index=False)\n",
    "dataframe.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "a7N3BKQD5EPp",
    "nDFE4-5S0INM",
    "dvZI8HcOkao7",
    "BD8lcaInkegE",
    "gb-epvsyPaPK",
    "NXLiczpOfNAJ",
    "uB_QOJk2Ib7k"
   ],
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
