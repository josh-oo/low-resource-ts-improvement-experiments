{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a7N3BKQD5EPp",
        "nDFE4-5S0INM",
        "dvZI8HcOkao7",
        "BD8lcaInkegE",
        "gb-epvsyPaPK",
        "NXLiczpOfNAJ",
        "uB_QOJk2Ib7k"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://dev:dtKN5sX9We7pw1soPB19@gitlab.lrz.de/josh-o/leichte-sprache-corpus.git"
      ],
      "metadata": {
        "id": "CriNJc7dem5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compute Controll Tokens"
      ],
      "metadata": {
        "id": "a7N3BKQD5EPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install sacremoses\n",
        "!pip install levenshtein"
      ],
      "metadata": {
        "id": "hoPuvLkSVYYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "w6gLNjLaAN6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.de.vec"
      ],
      "metadata": {
        "id": "Y8VdHxUITUdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute char ratio\n",
        "def char_ratio(row):\n",
        "  return len(row['simple_phrase']) / len(row['normal_phrase'])\n",
        "\n",
        "dataframe['nbchars'] = dataframe.apply(char_ratio, axis=1, result_type='reduce')"
      ],
      "metadata": {
        "id": "xSByXfpgWy9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute Levenshtein similarity\n",
        "\n",
        "import Levenshtein as lev\n",
        "\n",
        "def lev_ratio(row):\n",
        "  return lev.ratio(row['normal_phrase'], row['simple_phrase'])\n",
        "\n",
        "dataframe['lev_sim'] = dataframe.apply(lev_ratio, axis=1, result_type='reduce')"
      ],
      "metadata": {
        "id": "F0z2W6XQWMry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute mean sentence depth\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "def get_dependency_tree_depth(sentence):\n",
        "  def get_subtree_depth(node):\n",
        "      if len(list(node.children)) == 0:\n",
        "          return 0\n",
        "      return 1 + max([get_subtree_depth(child) for child in node.children])\n",
        "\n",
        "  tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in nlp(sentence).sents]\n",
        "\n",
        "  if len(tree_depths) == 0:\n",
        "      return 0\n",
        "  return np.mean(tree_depths)\n",
        "\n",
        "def dep_ratio(row):\n",
        "  return get_dependency_tree_depth(row['normal_phrase']) / get_dependency_tree_depth(row['simple_phrase'])\n",
        "\n",
        "dataframe['dep'] = dataframe.apply(dep_ratio, axis=1, result_type='reduce')"
      ],
      "metadata": {
        "id": "lninfboS-UY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute complexity\n",
        "\n",
        "from functools import lru_cache\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk_stopwords.words('german'))\n",
        "\n",
        "#FASTTEXT_EMBEDDINGS_PATH = \"/content/cc.de.300.vec.gz\"\n",
        "FASTTEXT_EMBEDDINGS_PATH = \"/content/wiki.de.vec\"\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([w for w in to_words(text) if w.lower() not in stopwords])\n",
        "\n",
        "def to_words(sentence):\n",
        "    return sentence.split()\n",
        "\n",
        "def remove_punctuation_characters(text):\n",
        "    return ''.join([char for char in text if char not in punctuation])\n",
        "\n",
        "def remove_punctuation_characters(text):\n",
        "    return ''.join([char for char in text if char not in punctuation])\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=1000)\n",
        "def is_punctuation(word):\n",
        "    return remove_punctuation_characters(word) == ''\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=100)\n",
        "def remove_punctuation_tokens(text):\n",
        "    return ' '.join([w for w in to_words(text) if not is_punctuation(w)])\n",
        "\n",
        "def count_lines(filepath):\n",
        "    n_lines = 0\n",
        "    with Path(filepath).open() as f:\n",
        "        for l in f:\n",
        "            n_lines += 1\n",
        "    return \n",
        "\n",
        "def yield_lines(filepath, n_lines=float('inf'), prop=1):\n",
        "    if prop < 1:\n",
        "        assert n_lines == float('inf')\n",
        "        n_lines = int(prop * count_lines(filepath))\n",
        "    with open(filepath, 'r') as f:\n",
        "        for i, l in enumerate(f):\n",
        "            if i >= n_lines:\n",
        "                break\n",
        "            yield l.rstrip('\\n')\n",
        "\n",
        "@lru_cache(maxsize=1)\n",
        "def get_word2rank(vocab_size=np.inf):\n",
        "    # TODO: Decrease vocab size or load from smaller file\n",
        "    word2rank = {}\n",
        "    line_generator = yield_lines(FASTTEXT_EMBEDDINGS_PATH)\n",
        "    next(line_generator)  # Skip the first line (header)\n",
        "    for i, line in enumerate(line_generator):\n",
        "        if (i + 1) > vocab_size:\n",
        "            break\n",
        "        word = line.split(' ')[0]\n",
        "        word2rank[word] = i\n",
        "    return word2rank\n",
        "\n",
        "\n",
        "def get_rank(word):\n",
        "    return get_word2rank().get(word, len(get_word2rank()))\n",
        "\n",
        "def get_log_rank(word):\n",
        "    return np.log(1 + get_rank(word))\n",
        "\n",
        "def get_lexical_complexity_score(sentence):\n",
        "    words = to_words(remove_stopwords(remove_punctuation_tokens(sentence)))\n",
        "    words = [word for word in words if word in get_word2rank()]\n",
        "    if len(words) == 0:\n",
        "        return np.log(1 + len(get_word2rank()))  # TODO: This is completely arbitrary\n",
        "    return np.quantile([get_log_rank(word) for word in words], 0.75)\n",
        "\n",
        "\n",
        "def complexity_ratio(row):\n",
        "  return  get_lexical_complexity_score(row['simple_phrase']) / get_lexical_complexity_score(row['normal_phrase'])\n",
        "\n",
        "dataframe['rank'] = dataframe.apply(complexity_ratio, axis=1, result_type='reduce')"
      ],
      "metadata": {
        "id": "ERc_N-QKPUB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.to_csv(\"valid_tokens.csv\", index=False)"
      ],
      "metadata": {
        "id": "iY9vaPrizsIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.describe()"
      ],
      "metadata": {
        "id": "ToQwbOHsGlpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare mlsum dataset"
      ],
      "metadata": {
        "id": "nDFE4-5S0INM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "3w-Zr8nWOnCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"mlsum\", \"de\")\n",
        "\n",
        "articles = dataset['train']['text'][:17100]\n",
        "data = {'normal_phrase' : articles, 'simple_phrase' : articles}\n",
        "\n",
        "regular_df = pd.DataFrame(data)\n",
        "regular_df.head()"
      ],
      "metadata": {
        "id": "Rhk6drmW0L3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare DeepL files\n",
        "As deepL requires .txt files with a maximum of 1 million characters, we split the csv fileinto .txt files of 1 million characters"
      ],
      "metadata": {
        "id": "gLQIl9l3xEf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir full_text"
      ],
      "metadata": {
        "id": "YsgWAuhcxMhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/20min/20min_aligned_train.csv\")\n",
        "#dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/20min/augmented/pure_simple_english_deepl.csv\")\n",
        "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/kurier_aligned_train.csv\")\n",
        "\n",
        "full_text = list(dataframe['normal_phrase'].values)\n",
        "\n",
        "i = 0\n",
        "while len(full_text) > 0:\n",
        "  total_length = 0\n",
        "  current_items = []\n",
        "  while len(full_text) > 0 and (total_length + len(full_text[0]) + 5 < 1_000_000):\n",
        "    current_item = full_text.pop(0)\n",
        "    current_items.append(current_item)\n",
        "    total_length += len(current_item) + 5\n",
        "  \n",
        "  full_text_part = \"\\n<#>\\n\".join(current_items)\n",
        "  print(f\"Part {i} has {total_length} chars\")\n",
        "  with open(f'full_text/part_{str(i).zfill(2)}.txt', 'w') as f:\n",
        "    f.write(full_text_part)\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "NrMe7x9AxJle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/full_text.zip /content/full_text"
      ],
      "metadata": {
        "id": "Z_ooeR5UxPPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install evaluate\n",
        "!pip install sacremoses\n",
        "!pip install sacrebleu==2.3.1"
      ],
      "metadata": {
        "id": "otJunvhAEjDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepL tp CSV\n",
        "Finally the .txt files are parsed back into the original .csv format."
      ],
      "metadata": {
        "id": "bVlalFXi3CCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = \"\"\n",
        "\n",
        "number_of_files = 10\n",
        "\n",
        "PDF_PREFIX = \"/content/\"\n",
        "\n",
        "parts = [str(i).zfill(2) for i in range(0, number_of_files)]\n",
        "for number in parts:\n",
        "  print(f\"Process {number}\")\n",
        "  file_path = (PDF_PREFIX + f'part_{number} de.txt')\n",
        "  file_str = open(file_path, 'r').read()\n",
        "  full_text += file_str.replace(\"\\n\",\"\")\n",
        "  full_text += \"<#>\"\n",
        "\n",
        "en_texts = full_text.split(\"<#>\")"
      ],
      "metadata": {
        "id": "FVooxIH5IwMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/kurier_aligned_train.csv\")\n",
        "\n",
        "with open('inputs_back_deepl.csv', 'w') as myfile:\n",
        "  myfile.write('normal_phrase,simple_phrase\\n')\n",
        "for i, en_text in enumerate(en_texts):\n",
        "  if en_text == \"\" and i == len(en_texts) -1:\n",
        "    continue\n",
        "  with open('inputs_back_deepl.csv', 'a') as myfile:\n",
        "    csvwriter = csv.writer(myfile)\n",
        "    normal_sample = en_text.strip()\n",
        "    simple_sample = dataframe.iloc[i]['simple_phrase']\n",
        "    csvwriter.writerow([normal_sample,simple_sample])"
      ],
      "metadata": {
        "id": "Iiof61jV5L2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv(\"inputs_back_deepl.csv\")\n",
        "dataframe.tail()"
      ],
      "metadata": {
        "id": "khIQhlmnScNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Google Translate Files\n",
        "Same thing as for deepL, but Google allows excel files which is more convenient"
      ],
      "metadata": {
        "id": "OifSDvwDRy58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir full_text"
      ],
      "metadata": {
        "id": "ogfEV8p_0Z21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#to excel\n",
        "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/augmented/inputs_english_deepl.csv\")\n",
        "cropped_df = dataframe.copy()['normal_phrase']\n",
        "\n",
        "splits = 1\n",
        "\n",
        "part_length = int(len(cropped_df.index)/splits)\n",
        "\n",
        "for i in range(0, splits):\n",
        "  with pd.ExcelWriter(f'full_text/output_{str(i).zfill(2)}.xlsx') as writer:  \n",
        "    cropped_df[i*part_length:(i+1)*part_length].to_excel(writer, sheet_name='Sheet_1')\n",
        "\n",
        "if part_length*splits < len(cropped_df.index):\n",
        "  with pd.ExcelWriter(f'full_text/output_{str(splits).zfill(2)}.xlsx') as writer:  \n",
        "    cropped_df[splits*part_length:].to_excel(writer, sheet_name='Sheet_1')"
      ],
      "metadata": {
        "id": "A04xA__sR19_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google -> CSV"
      ],
      "metadata": {
        "id": "BRnp0q2ie-eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.formats.format import DataFrameFormatter\n",
        "full_text = \"\"\n",
        "\n",
        "number_of_files = 1\n",
        "\n",
        "PDF_PREFIX = \"/content/\"\n",
        "\n",
        "all_dataframes = []\n",
        "\n",
        "parts = [str(i).zfill(2) for i in range(0, number_of_files)]\n",
        "for number in parts:\n",
        "  print(f\"Process {number}\")\n",
        "  file_path = (PDF_PREFIX + f'output_{number}_de.xlsx')\n",
        "  current_dataframe = pd.read_excel(file_path, index_col=0)  \n",
        "  all_dataframes.append(current_dataframe)\n",
        "\n",
        "back_translated_df = pd.concat(all_dataframes)\n",
        "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/kurier_aligned_train.csv\")\n",
        "dataframe['normal_phrase'] = back_translated_df['normale_phrase']\n",
        "dataframe.to_csv('pure_simple_back_google.csv') "
      ],
      "metadata": {
        "id": "DKfToP6GfBaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pure Simple\n"
      ],
      "metadata": {
        "id": "dvZI8HcOkao7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/20min/20min_aligned_train.csv\")\n",
        "dataframe['normal_phrase'] = dataframe['simple_phrase']\n",
        "dataframe.to_csv(\"pure_simple.csv\", index=False)\n",
        "dataframe.head()"
      ],
      "metadata": {
        "id": "F8n0I324B7Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Add Simple Noise"
      ],
      "metadata": {
        "id": "NXLiczpOfNAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/valentinmace/noisy-text.git noisy_text"
      ],
      "metadata": {
        "id": "n_fYbNaKfSDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/noisy_text')"
      ],
      "metadata": {
        "id": "FrrWPAwBhWZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from noise_functions import delete_random_token, replace_random_token, random_token_permutation\n",
        "\n",
        "def add_noise(line):\n",
        "  line = delete_random_token(line, probability=0.1)\n",
        "  line = replace_random_token(line, probability=0.1, filler_token=\"<mask>\")\n",
        "  line = random_token_permutation(line, _range=3)\n",
        "  return line"
      ],
      "metadata": {
        "id": "33IgJPhgkqDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/kurier/augmented/inputs_back_google.csv\")\n",
        "dataframe['normal_phrase'] = dataframe['normal_phrase'].apply(add_noise)\n",
        "dataframe.to_csv(\"../inputs_back_google_simple_noise.csv\", index=False)\n",
        "dataframe.tail()"
      ],
      "metadata": {
        "id": "nHvnBYoglHlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Add Bart Noise"
      ],
      "metadata": {
        "id": "uB_QOJk2Ib7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "1LRePSuWXtNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartTokenizer\n",
        "\n",
        "tokenizer =  MBartTokenizer.from_pretrained(\"josh-oo/modified-mbart\")"
      ],
      "metadata": {
        "id": "Pv2lgf_fYTyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adapted from https://github.com/facebookresearch/fairseq/blob/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/data/denoising_dataset.py#L257\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from random import randrange\n",
        "\n",
        "def poison_distribution(poisson_lambda=3):\n",
        "  _lambda = poisson_lambda\n",
        "\n",
        "  lambda_to_the_k = 1\n",
        "  e_to_the_minus_lambda = math.exp(-_lambda)\n",
        "  k_factorial = 1\n",
        "  ps = []\n",
        "  for k in range(0, 128):\n",
        "      ps.append(e_to_the_minus_lambda * lambda_to_the_k / k_factorial)\n",
        "      lambda_to_the_k *= _lambda\n",
        "      k_factorial *= k + 1\n",
        "      if ps[-1] < 0.0000001:\n",
        "          break\n",
        "  ps = torch.FloatTensor(ps)\n",
        "  return torch.distributions.Categorical(ps)\n",
        "\n",
        "def split_into_sentences(text):\n",
        "  # Zerlegung des Textes in Sätze nach diesen Regeln:\n",
        "  # (?<!\\.\\.)\\s -> Keine Trennung bei ...\n",
        "  # (?<!\\w\\.\\w.)\\s \t\t-> Keine Trennung wenn zwei Zeichen mit einem Punkt in der Mitte und am Ende dem Leerzeichen vorausgehen (z.B. etc.)\n",
        "  # (?<![0-9]\\.)\\s \t\t-> Keine Trennung wenn eine Nummer folgend von einem Punkt dem Leerzeichen vorausgeht (9. etc.)\n",
        "  # (?<![0-9][0-9]\\.)\\s \t-> Keine Trennung wenn zwei Nummern folgend von einem Punkt dem Leerzeichen vorausgehen (18. etc.)\n",
        "  # (?<![A-Z]\\.)\t\t\t-> Keine Trennung wenn ein Großbuchstabe gefolgt von einem Punkt dem Leerzeichen vorausgehen (W. etc)\n",
        "  # (?<![A-Z][a-z]\\.)\\s \t-> Keine Trennung wenn ein Großbuchstabe gefolgt von einem Kleibuchstaben und einem Punkt dem Leerzeichen vorausgehen (Dr. etc)\n",
        "  # (?<=\\.|\\?|\\!)\\s \t\t-> Trennen wenn ein Punkt, Fragezeichen oder Ausrufezeichen dem Leerzeichen vorausgehen\n",
        "  \n",
        "  sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![0-9]\\.)(?<![0-9][0-9]\\.)(?<![A-Z]\\.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\", text)\n",
        "  sentences = [s for s in sentences if s]\n",
        "  return sentences\n",
        "\n",
        "\n",
        "def permute_sentences(line, p=1.0):\n",
        "  sentences = split_into_sentences(line)\n",
        "\n",
        "  result = sentences.copy()\n",
        "\n",
        "  num_sentences = len(sentences)\n",
        "  num_to_permute = math.ceil((num_sentences * 2 * p) / 2.0)\n",
        "  substitutions = torch.randperm(num_sentences)[:num_to_permute]\n",
        "  ordering = torch.arange(0, num_sentences)\n",
        "  ordering[substitutions] = substitutions[torch.randperm(num_to_permute)]\n",
        "\n",
        "  for result_index, i in enumerate(ordering):\n",
        "    result[result_index] = sentences[i]\n",
        "  return \" \".join(result)\n",
        "\n",
        "def get_random_token():\n",
        "  while True:\n",
        "    index = randrange(tokenizer.vocab_size)\n",
        "    out = tokenizer.decode([index])\n",
        "    if len(tokenizer([out])['input_ids'][0]) == 3: #TODO adapt for other tokenizers\n",
        "      return out\n",
        "\n",
        "def add_whole_word_mask(line, p=0.3, random_ratio=0.1):\n",
        "  words = line.split(\" \")\n",
        "  num_to_mask = int(math.ceil(len(words) * p))\n",
        "  num_inserts = 0\n",
        "  if num_to_mask == 0:\n",
        "      return line\n",
        "\n",
        "  mask_span_distribution = poison_distribution(poisson_lambda=1.8)\n",
        "  lengths = mask_span_distribution.sample(sample_shape=(num_to_mask,))\n",
        "  # Make sure we have enough to mask\n",
        "  cum_length = torch.cumsum(lengths, 0)\n",
        "  while cum_length[-1] < num_to_mask:\n",
        "      lengths = torch.cat(\n",
        "          [\n",
        "              lengths,\n",
        "              mask_span_distribution.sample(sample_shape=(num_to_mask,)),\n",
        "          ],\n",
        "          dim=0,\n",
        "      )\n",
        "      cum_length = torch.cumsum(lengths, 0)\n",
        "\n",
        "  # Trim to masking budget\n",
        "  i = 0\n",
        "  while cum_length[i] < num_to_mask:\n",
        "      i += 1\n",
        "  lengths[i] = num_to_mask - (0 if i == 0 else cum_length[i - 1])\n",
        "  num_to_mask = i + 1\n",
        "  lengths = lengths[:num_to_mask]\n",
        "\n",
        "  # Handle 0-length mask (inserts) separately\n",
        "  lengths = lengths[lengths > 0]\n",
        "  num_inserts = num_to_mask - lengths.size(0)\n",
        "  num_to_mask -= num_inserts\n",
        "  if num_to_mask == 0:\n",
        "      return \" \".join(words)\n",
        "\n",
        "  assert (lengths > 0).all()\n",
        "  indices = torch.randperm(len(words))[:num_to_mask]\n",
        "  mask_random = torch.FloatTensor(num_to_mask).uniform_() < random_ratio\n",
        "  source_length = len(words)\n",
        "\n",
        "  #to_keep = torch.ones(source_length, dtype=torch.bool)\n",
        "  # keep index, but replace it with [MASK]\n",
        "  index_to_remove = []\n",
        "  to_keep = torch.ones(len(words), dtype=torch.bool)\n",
        "  for i, index in enumerate(indices):\n",
        "    words[index] = \"<mask>\"\n",
        "    if mask_random[i]:\n",
        "      words[index] = get_random_token()\n",
        "    current_length = lengths[i]\n",
        "    for shift in range(1,current_length):\n",
        "      if index + shift < len(to_keep):\n",
        "        to_keep[index + shift] = False\n",
        "\n",
        "  assert len(lengths.size()) == 1\n",
        "  assert lengths.size() == indices.size()\n",
        "  \n",
        "  words = np.array(words)[to_keep]\n",
        "\n",
        "  return \" \".join(words)\n",
        "\n",
        "def crop_texts(line, max_length=512):\n",
        "  tokens = tokenizer.tokenize(line)[:max_length]\n",
        "  return tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "def add_bart_noise(line, permutation=1.0, masking=0.3, random_masking=0.1):\n",
        "  if permutation > 0:\n",
        "    line = permute_sentences(line, permutation)\n",
        "  if masking > 0:\n",
        "    line = add_whole_word_mask(line, masking, random_masking)\n",
        "  return line"
      ],
      "metadata": {
        "id": "UOshkuJoJsqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.read_csv(\"/content/leichte-sprache-corpus/aligned/20min/20min_aligned_train.csv\")\n",
        "#dataframe['normal_phrase'] = dataframe['normal_phrase'].apply(crop_texts)\n",
        "dataframe['normal_phrase'] = dataframe['normal_phrase'].apply(add_bart_noise)\n",
        "dataframe.to_csv(\"inputs_back_bart_noise.csv\", index=False)\n",
        "dataframe.head()"
      ],
      "metadata": {
        "id": "3cDFCTD4BCf6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}