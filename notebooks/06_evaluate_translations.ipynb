{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yVT2Dt5vIcb"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#install dependencies (April 14, 2023)\n",
    "!pip install sacrebleu==2.3.1 \n",
    "!pip install evaluate==0.3.0 \n",
    "!pip install rouge_score==0.1.2 \n",
    "!pip install sacremoses==0.0.53 \n",
    "!pip install textstat==0.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20min da\n",
    "#systems = ['20min-dropout-0_0-0_1-most-similar.txt', '20min-dropout-0_1-0_1-most-similar.txt', '20min-dropout-0_3-0_1-most-similar.txt', '20min-dropout-0_8-0_1-most-similar.txt', '20min-backtranslation-0_0-0_1-most-similar.txt', '20min-simple_noise-0_0-0_1-most-similar.txt', '20min-bart_noise-0_0-0_1-most-similar.txt', '20min-bt_noise-0_0-0_1-most-similar.txt','20min-english-0_0-0_1-most-similar.txt']\n",
    "\n",
    "#Kurier ft decoder\n",
    "#systems = ['Kurier-mbart-0_1-0_1-most-similar.txt','Kurier-ft-decoder-mbart-0_1-0_1-most-similar.txt', 'Kurier-ft-decoder-gaussian-noise-mbart-0_1-0_1-most-similar.txt', 'Kurier-ft-decoder-bart-noise-mbart-0_1-0_1-most-similar.txt'] \n",
    "\n",
    "#20min retrained cross attentions\n",
    "systems = ['20min-regular-mbart-0_1-0_1-most-similar.txt','20min-random-mbart-0_1-0_1-most-similar.txt', '20min-retrained-mbart-0_1-0_1-most-similar.txt', '20min-distilled[-1]-mbart-0_1-0_1-most-similar.txt', '20min-distilled[r,r,-1]-mbart-0_1-0_1-most-similar.txt'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "\n",
    "This notebook is primarly intended to be run on Google Colab.  \n",
    "You need to place your *refs.txt* und *sources.txt* files for the corresponding translations in this notebook's directory.  \n",
    "Place the translations candidates you want to test in the same directory and reference them in the *systems* array in the cell above.  \n",
    "The *systems* array's first item is the baseline system. All significance values are calculated pairwise against this system.  \n",
    "\n",
    "When the above steps have been performed, all cells can be executed consecutively. The results are then presented in the penultimate cell in the form of a python dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PzgvRuSzYWB"
   },
   "source": [
    "# Set up custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePjd7ZyQbySN"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from importlib import import_module\n",
    "from typing import List, Sequence, Optional, Dict, Any\n",
    "\n",
    "from sacrebleu.utils import my_log, sum_of_lists\n",
    "\n",
    "from sacrebleu.metrics.base import Score, Signature, Metric\n",
    "from sacrebleu.metrics.helpers import extract_all_word_ngrams\n",
    "\n",
    "import textstat\n",
    "\n",
    "class FRESignature(Signature):\n",
    "    \"\"\"A convenience class to represent the reproducibility signature for FRE.\n",
    "\n",
    "    :param args: key-value dictionary passed from the actual metric instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: dict):\n",
    "        \"\"\"`FRESignature` initializer.\"\"\"\n",
    "        super().__init__(args)\n",
    "\n",
    "\n",
    "class FREScore(Score):\n",
    "    \"\"\"A convenience class to represent FRE scores.\n",
    "\n",
    "    :param score: The FRE score.\n",
    "    \"\"\"\n",
    "    def __init__(self, score: float):\n",
    "        \"\"\"`FREScore` initializer.\"\"\"\n",
    "        super().__init__('FRE', score)\n",
    "\n",
    "\n",
    "class FRE(Metric):\n",
    "    \"\"\"Computes the FRE metric given hypotheses and references.\n",
    "    \"\"\"\n",
    "\n",
    "    _SIGNATURE_TYPE = FRESignature\n",
    "\n",
    "    def __init__(self, references: Optional[Sequence[Sequence[str]]] = None):\n",
    "        \"\"\"`FRE` initializer.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        textstat.set_lang(\"de\")\n",
    "\n",
    "        if references is not None:\n",
    "            # Pre-compute reference ngrams and lengths\n",
    "            self._ref_cache = self._cache_references(references)\n",
    "\n",
    "    def _preprocess_segment(self, sent: str) -> str:\n",
    "        \"\"\"Given a sentence, lowercases (optionally) and tokenizes it\n",
    "        :param sent: The input sentence string.\n",
    "        :return: The pre-processed output string.\n",
    "        \"\"\"\n",
    "        return sent\n",
    "\n",
    "    def _compute_score_from_stats(self, stats: List[float]) -> FREScore:\n",
    "        \"\"\"Computes the final score from already aggregated statistics.\n",
    "\n",
    "        :param stats: A list or numpy array of segment-level statistics.\n",
    "        :return: A `RougeScore` object.\n",
    "        \"\"\"\n",
    "        return FREScore(stats[0] / stats[1])\n",
    "\n",
    "    def _aggregate_and_compute(self, stats: List[List[int]]) -> FREScore:\n",
    "        \"\"\"Computes the final BLEU score given the pre-computed corpus statistics.\n",
    "\n",
    "        :param stats: A list of segment-level statistics\n",
    "        :return: A `RougeScore` instance.\n",
    "        \"\"\"\n",
    "        return self._compute_score_from_stats(sum_of_lists(stats))\n",
    "\n",
    "    def _extract_reference_info(self, refs: Sequence[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Given a list of reference segments, extract the n-grams and reference lengths.\n",
    "        The latter will be useful when comparing hypothesis and reference lengths for BLEU.\n",
    "\n",
    "        :param refs: A sequence of strings.\n",
    "        :return: A dictionary that will be passed to `_compute_segment_statistics()`\n",
    "        through keyword arguments.\n",
    "        \"\"\"\n",
    "        return {'refs': refs}\n",
    "\n",
    "    def _compute_segment_statistics(self, hypothesis: str,\n",
    "                                    ref_kwargs: Dict) -> List[int]:\n",
    "        \"\"\"Given a (pre-processed) hypothesis sentence and already computed\n",
    "        reference n-grams & lengths, returns the best match statistics across the\n",
    "        references.\n",
    "\n",
    "        :param hypothesis: Hypothesis sentence.\n",
    "        :param ref_kwargs: A dictionary with `refs_ngrams`and `ref_lens` keys\n",
    "        that denote the counter containing all n-gram counts and reference lengths,\n",
    "        respectively.\n",
    "        :return: A list of integers with match statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        refs = ref_kwargs['refs']\n",
    "        \n",
    "        fre_score = max(0,textstat.flesch_reading_ease(hypothesis))\n",
    "        #fre_score, 1 to count the total lines\n",
    "        return [fre_score, 1 ]\n",
    "\n",
    "    def sentence_score(self, hypothesis: str, references: Sequence[str]) -> FREScore:\n",
    "        \"\"\"Compute the metric for a single sentence against a single (or multiple) reference(s).\n",
    "\n",
    "        :param hypothesis: A single hypothesis string.\n",
    "        :param references: A sequence of reference strings.\n",
    "        :return: a `FREScore` object.\n",
    "        \"\"\"\n",
    "        fre_score = max(0,textstat.flesch_reading_ease(hypothesis))\n",
    "        return FREScore(fre_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bG29VDd52RKo"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from importlib import import_module\n",
    "from typing import List, Sequence, Optional, Dict, Any\n",
    "\n",
    "from sacrebleu.utils import my_log, sum_of_lists\n",
    "\n",
    "from sacrebleu.metrics.base import Score, Signature, Metric\n",
    "from sacrebleu.metrics.helpers import extract_all_word_ngrams\n",
    "\n",
    "import evaluate\n",
    "\n",
    "class RougeSignature(Signature):\n",
    "    \"\"\"A convenience class to represent the reproducibility signature for Rouge.\n",
    "\n",
    "    :param args: key-value dictionary passed from the actual metric instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: dict):\n",
    "        \"\"\"`RougeSignature` initializer.\"\"\"\n",
    "        super().__init__(args)\n",
    "\n",
    "\n",
    "class RougeScore(Score):\n",
    "    \"\"\"A convenience class to represent Rouge scores.\n",
    "\n",
    "    :param score: The Rouge score.\n",
    "    \"\"\"\n",
    "    def __init__(self, score: float):\n",
    "        \"\"\"`RougeScore` initializer.\"\"\"\n",
    "        super().__init__('Rouge', score)\n",
    "\n",
    "\n",
    "class ROUGE(Metric):\n",
    "    \"\"\"Computes the Rouge metric given hypotheses and references.\n",
    "    \"\"\"\n",
    "\n",
    "    _SIGNATURE_TYPE = RougeSignature\n",
    "\n",
    "    def __init__(self, references: Optional[Sequence[Sequence[str]]] = None):\n",
    "        \"\"\"`Rouge` initializer.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.rouge = evaluate.load('rouge')\n",
    "\n",
    "        if references is not None:\n",
    "            # Pre-compute reference ngrams and lengths\n",
    "            self._ref_cache = self._cache_references(references)\n",
    "\n",
    "    def _preprocess_segment(self, sent: str) -> str:\n",
    "        \"\"\"Given a sentence, lowercases (optionally) and tokenizes it\n",
    "        :param sent: The input sentence string.\n",
    "        :return: The pre-processed output string.\n",
    "        \"\"\"\n",
    "        return sent\n",
    "\n",
    "    def _compute_score_from_stats(self, stats: List[float]) -> RougeScore:\n",
    "        \"\"\"Computes the final score from already aggregated statistics.\n",
    "\n",
    "        :param stats: A list or numpy array of segment-level statistics.\n",
    "        :return: A `RougeScore` object.\n",
    "        \"\"\"\n",
    "        return RougeScore(stats[0] / stats[1])\n",
    "\n",
    "    def _aggregate_and_compute(self, stats: List[List[int]]) -> RougeScore:\n",
    "        \"\"\"Computes the final BLEU score given the pre-computed corpus statistics.\n",
    "\n",
    "        :param stats: A list of segment-level statistics\n",
    "        :return: A `RougeScore` instance.\n",
    "        \"\"\"\n",
    "        return self._compute_score_from_stats(sum_of_lists(stats))\n",
    "\n",
    "    def _extract_reference_info(self, refs: Sequence[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Given a list of reference segments, extract the n-grams and reference lengths.\n",
    "        The latter will be useful when comparing hypothesis and reference lengths for BLEU.\n",
    "\n",
    "        :param refs: A sequence of strings.\n",
    "        :return: A dictionary that will be passed to `_compute_segment_statistics()`\n",
    "        through keyword arguments.\n",
    "        \"\"\"\n",
    "        return {'refs': refs}\n",
    "\n",
    "    def _compute_segment_statistics(self, hypothesis: str,\n",
    "                                    ref_kwargs: Dict) -> List[int]:\n",
    "        \"\"\"Given a (pre-processed) hypothesis sentence and already computed\n",
    "        reference n-grams & lengths, returns the best match statistics across the\n",
    "        references.\n",
    "\n",
    "        :param hypothesis: Hypothesis sentence.\n",
    "        :param ref_kwargs: A dictionary with `refs_ngrams`and `ref_lens` keys\n",
    "        that denote the counter containing all n-gram counts and reference lengths,\n",
    "        respectively.\n",
    "        :return: A list of integers with match statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        refs = ref_kwargs['refs']\n",
    "        \n",
    "        #use use_aggregator=False to avoid duplicated bootstrap resampling\n",
    "        rouge_score = self.rouge.compute(predictions=[hypothesis],references=refs, use_aggregator=False)\n",
    "        #rouge_score, 1 to count the total lines\n",
    "        return [rouge_score['rougeL'][0], 1 ]\n",
    "\n",
    "    def sentence_score(self, hypothesis: str, references: Sequence[str]) -> RougeScore:\n",
    "        \"\"\"Compute the metric for a single sentence against a single (or multiple) reference(s).\n",
    "\n",
    "        :param hypothesis: A single hypothesis string.\n",
    "        :param references: A sequence of reference strings.\n",
    "        :return: a `RougeScore` object.\n",
    "        \"\"\"\n",
    "        #use use_aggregator=False to avoid duplicated bootstrap resampling\n",
    "        rouge_score = self.rouge.compute(predictions=[hypothesis], references=references, use_aggregator=False)\n",
    "        return RougeScore(rouge_score['rougeL'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_GPn5_inTjy"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from importlib import import_module\n",
    "from typing import List, Sequence, Optional, Dict, Any\n",
    "\n",
    "from sacrebleu.utils import my_log, sum_of_lists\n",
    "\n",
    "from sacrebleu.metrics.base import Score, Signature, Metric\n",
    "from sacrebleu.metrics.helpers import extract_all_word_ngrams\n",
    "\n",
    "import evaluate\n",
    "\n",
    "class SARISignature(Signature):\n",
    "    \"\"\"A convenience class to represent the reproducibility signature for SARI.\n",
    "\n",
    "    :param args: key-value dictionary passed from the actual metric instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: dict):\n",
    "        \"\"\"`SARISignature` initializer.\"\"\"\n",
    "        super().__init__(args)\n",
    "\n",
    "\n",
    "class SARIScore(Score):\n",
    "    \"\"\"A convenience class to represent SARI scores.\n",
    "\n",
    "    :param score: The SARI score.\n",
    "    \"\"\"\n",
    "    def __init__(self, score: float):\n",
    "        \"\"\"`SARIScore` initializer.\"\"\"\n",
    "        super().__init__('SARI', score)\n",
    "\n",
    "\n",
    "class SARI(Metric):\n",
    "    \"\"\"Computes the SARI metric given hypotheses and references.\n",
    "    \"\"\"\n",
    "\n",
    "    _SIGNATURE_TYPE = SARISignature\n",
    "\n",
    "    def __init__(self, sources , references: Optional[Sequence[Sequence[str]]] = None):\n",
    "        \"\"\"`SARI` initializer.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.sari = evaluate.load('sari')\n",
    "        self.sources = sources\n",
    "\n",
    "        if references is not None:\n",
    "            # Pre-compute reference ngrams and lengths\n",
    "            self._ref_cache = self._cache_references(references)\n",
    "            print(\"hit\")\n",
    "\n",
    "    def _preprocess_segment(self, sent: str) -> str:\n",
    "        \"\"\"Given a sentence, lowercases (optionally) and tokenizes it\n",
    "        :param sent: The input sentence string.\n",
    "        :return: The pre-processed output string.\n",
    "        \"\"\"\n",
    "        return sent\n",
    "\n",
    "    def _compute_score_from_stats(self, stats: List[float]) -> SARIScore:\n",
    "        \"\"\"Computes the final score from already aggregated statistics.\n",
    "\n",
    "        :param stats: A list or numpy array of segment-level statistics.\n",
    "        :return: A `SARIScore` object.\n",
    "        \"\"\"\n",
    "        return SARIScore(stats[0] / stats[1])\n",
    "\n",
    "    def _aggregate_and_compute(self, stats: List[List[int]]) -> SARIScore:\n",
    "        \"\"\"Computes the final BLEU score given the pre-computed corpus statistics.\n",
    "\n",
    "        :param stats: A list of segment-level statistics\n",
    "        :return: A `SARIScore` instance.\n",
    "        \"\"\"\n",
    "        return self._compute_score_from_stats(sum_of_lists(stats))\n",
    "\n",
    "    def _extract_reference_info(self, refs: Sequence[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Given a list of reference segments, extract the n-grams and reference lengths.\n",
    "        The latter will be useful when comparing hypothesis and reference lengths for BLEU.\n",
    "\n",
    "        :param refs: A sequence of strings.\n",
    "        :return: A dictionary that will be passed to `_compute_segment_statistics()`\n",
    "        through keyword arguments.\n",
    "        \"\"\"\n",
    "        return {'refs': refs, 'sources': self.sources}\n",
    "\n",
    "    def _compute_segment_statistics(self, hypothesis: str,\n",
    "                                    ref_kwargs: Dict) -> List[int]:\n",
    "        \"\"\"Given a (pre-processed) hypothesis sentence and already computed\n",
    "        reference n-grams & lengths, returns the best match statistics across the\n",
    "        references.\n",
    "\n",
    "        :param hypothesis: Hypothesis sentence.\n",
    "        :param ref_kwargs: A dictionary with `refs_ngrams`and `ref_lens` keys\n",
    "        that denote the counter containing all n-gram counts and reference lengths,\n",
    "        respectively.\n",
    "        :return: A list of integers with match statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        refs = ref_kwargs['refs']\n",
    "        sources = ref_kwargs['sources'][hash(refs[0])]\n",
    "\n",
    "        sari_score = self.sari.compute(sources=[sources], predictions=[hypothesis],references=[refs])\n",
    "        #rouge_score, 1 to count the total lines\n",
    "        return [sari_score['sari'], 1 ]\n",
    "\n",
    "    def sentence_score(self, hypothesis: str, references: Sequence[str]) -> SARIScore:\n",
    "        \"\"\"Compute the metric for a single sentence against a single (or multiple) reference(s).\n",
    "\n",
    "        :param hypothesis: A single hypothesis string.\n",
    "        :param references: A sequence of reference strings.\n",
    "        :return: a `RougeScore` object.\n",
    "        \"\"\"\n",
    "        sari_score = self.sari.compute(predictions=[hypothesis], references=references)\n",
    "        return RougeScore(sari_score['sari'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiuuSf2YYilk"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from importlib import import_module\n",
    "from typing import List, Sequence, Optional, Dict, Any\n",
    "\n",
    "from sacrebleu.utils import my_log, sum_of_lists\n",
    "\n",
    "from sacrebleu.metrics.base import Score, Signature, Metric\n",
    "from sacrebleu.metrics.helpers import extract_all_word_ngrams\n",
    "\n",
    "class REPETITIONSignature(Signature):\n",
    "    \"\"\"A convenience class to represent the reproducibility signature for RepetitionScore.\n",
    "\n",
    "    :param args: key-value dictionary passed from the actual metric instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: dict):\n",
    "        \"\"\"`RepetitionSignature` initializer.\"\"\"\n",
    "        super().__init__(args)\n",
    "\n",
    "\n",
    "class REPETITIONScore(Score):\n",
    "    \"\"\"A convenience class to represent Repetition scores.\n",
    "\n",
    "    :param score: The Repetition score.\n",
    "    \"\"\"\n",
    "    def __init__(self, score: float):\n",
    "        \"\"\"`RepetitionScore` initializer.\"\"\"\n",
    "        super().__init__('Repetition', score)\n",
    "\n",
    "\n",
    "class REPETITION(Metric):\n",
    "    \"\"\"Computes the Rouge metric given hypotheses and references.\n",
    "    \"\"\"\n",
    "\n",
    "    _SIGNATURE_TYPE = REPETITIONSignature\n",
    "\n",
    "    def __init__(self, references: Optional[Sequence[Sequence[str]]] = None):\n",
    "        \"\"\"`Repetition` initializer.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def _process_line(self, line, min_repetitions=2):\n",
    "        max_intersection = []\n",
    "        line = line.strip()\n",
    "        for i in range(min_repetitions, len(line)):\n",
    "          longest_intersections = 0\n",
    "          line_copy = line[i:] + line[:i]\n",
    "          current_row = 0\n",
    "          for x, y in zip(line, line_copy):\n",
    "            if(x != y):\n",
    "              if current_row > longest_intersections:\n",
    "                longest_intersections = current_row\n",
    "              current_row = 0\n",
    "            else:\n",
    "              current_row += 1\n",
    "          if longest_intersections >= i:\n",
    "            max_intersection.append(longest_intersections)\n",
    "\n",
    "        if len(max_intersection) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "          return max(max_intersection) / len(line)\n",
    "\n",
    "    def _preprocess_segment(self, sent: str) -> str:\n",
    "        \"\"\"Given a sentence, lowercases (optionally) and tokenizes it\n",
    "        :param sent: The input sentence string.\n",
    "        :return: The pre-processed output string.\n",
    "        \"\"\"\n",
    "        return sent\n",
    "\n",
    "    def _compute_score_from_stats(self, stats: List[float]) -> REPETITIONScore:\n",
    "        \"\"\"Computes the final score from already aggregated statistics.\n",
    "\n",
    "        :param stats: A list or numpy array of segment-level statistics.\n",
    "        :return: A `RepetitionScore` object.\n",
    "        \"\"\"\n",
    "        return REPETITIONScore(stats[0] / stats[1])\n",
    "\n",
    "    def _aggregate_and_compute(self, stats: List[List[int]]) -> REPETITIONScore:\n",
    "        \"\"\"Computes the final Repetition score given the pre-computed corpus statistics.\n",
    "\n",
    "        :param stats: A list of segment-level statistics\n",
    "        :return: A `RepetitionScore` instance.\n",
    "        \"\"\"\n",
    "        return self._compute_score_from_stats(sum_of_lists(stats))\n",
    "\n",
    "    def _extract_reference_info(self, refs: Sequence[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Given a list of reference segments, extract the n-grams and reference lengths.\n",
    "        The latter will be useful when comparing hypothesis and reference lengths for BLEU.\n",
    "\n",
    "        :param refs: A sequence of strings.\n",
    "        :return: A dictionary that will be passed to `_compute_segment_statistics()`\n",
    "        through keyword arguments.\n",
    "        \"\"\"\n",
    "        return {'refs': refs}\n",
    "\n",
    "    def _compute_segment_statistics(self, hypothesis: str,\n",
    "                                    ref_kwargs: Dict) -> List[int]:\n",
    "        \"\"\"Given a (pre-processed) hypothesis sentence and already computed\n",
    "        reference n-grams & lengths, returns the best match statistics across the\n",
    "        references.\n",
    "\n",
    "        :param hypothesis: Hypothesis sentence.\n",
    "        :param ref_kwargs: A dictionary with `refs_ngrams`and `ref_lens` keys\n",
    "        that denote the counter containing all n-gram counts and reference lengths,\n",
    "        respectively.\n",
    "        :return: A list of integers with match statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        result = self._process_line(hypothesis)\n",
    "        return [result, 1 ]\n",
    "\n",
    "    def sentence_score(self, hypothesis: str, references: Sequence[str]) -> REPETITIONScore:\n",
    "        \"\"\"Compute the metric for a single sentence against a single (or multiple) reference(s).\n",
    "\n",
    "        :param hypothesis: A single hypothesis string.\n",
    "        :param references: A sequence of reference strings.\n",
    "        :return: a `RougeScore` object.\n",
    "        \"\"\"\n",
    "        result = self._process_line(hypothesis)\n",
    "        return  REPETITIONScore(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axzYIfMpzdOL"
   },
   "source": [
    "# Evaluate Systems\n",
    "\n",
    "Drag and drop refs.txt, sources.txt and your corresponding translation .txt (one for each system) to the left sidebar of google colab and run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ULyhoJagaqC"
   },
   "outputs": [],
   "source": [
    "#adapted from sacrebleu\n",
    "\n",
    "from sacrebleu.significance import PairedTest\n",
    "from sacrebleu.utils import filter_subset, smart_open\n",
    "from sacrebleu.metrics.bleu import BLEU\n",
    "\n",
    "refs_file = \"refs.txt\"\n",
    "sources_file = 'sources.txt'\n",
    "\n",
    "paired_bs = True\n",
    "paired_bs_n = 1000\n",
    "paired_ar_n = 10000\n",
    "short = False\n",
    "paired_jobs = 1\n",
    "\n",
    "# Set params\n",
    "test_type = 'bs' if paired_bs else 'ar'\n",
    "n_samples = paired_bs_n if paired_bs else paired_ar_n\n",
    "\n",
    "full_systems, sys_names = [], []\n",
    "\n",
    "test_set = None\n",
    "langpair = None\n",
    "origlang = None\n",
    "subset = None\n",
    "\n",
    "concat_ref_files = [[refs_file]]\n",
    "num_refs = len(concat_ref_files)\n",
    "\n",
    "references = []\n",
    "with open(refs_file) as source_file:\n",
    "  references = source_file.readlines()\n",
    "sources = {}\n",
    "with open(sources_file) as source_file:\n",
    "  for i,line in enumerate(source_file.readlines()):\n",
    "      sources[hash(references[i].strip())] = line\n",
    "\n",
    "metrics = {\n",
    "    'BLEU': BLEU(), \n",
    "    'RougeL': ROUGE(),\n",
    "    'SARI': SARI(sources=sources),\n",
    "    'FRE': FRE(),\n",
    "    'REPETITIONS': REPETITION(), \n",
    "    }\n",
    "\n",
    "for fname in systems:\n",
    "  sys_name = fname\n",
    "\n",
    "  # Read the system\n",
    "  lines = []\n",
    "  for line in smart_open(fname, encoding='utf-8'):\n",
    "      lines.append(line.rstrip())\n",
    "  full_systems.append(lines)\n",
    "  sys_names.append(sys_name)\n",
    "\n",
    "num_sys = len(sys_names)\n",
    "\n",
    "full_refs = [[] for x in range(max(len(concat_ref_files[0]), num_refs))]\n",
    "for ref_files in concat_ref_files:\n",
    "    for refno, ref_file in enumerate(ref_files):\n",
    "      for lineno, line in enumerate(smart_open(ref_file, encoding='utf-8'), 1):\n",
    "          line = line.rstrip()\n",
    "          if num_refs == 1:\n",
    "              full_refs[refno].append(line)\n",
    "\n",
    "# Filter subsets if requested\n",
    "outputs = filter_subset(\n",
    "    [*full_systems, *full_refs], test_set, langpair,\n",
    "    origlang, subset)\n",
    "\n",
    "# Unpack systems & references back\n",
    "systems, refs = outputs[:num_sys], outputs[num_sys:]\n",
    "\n",
    "named_systems = [(sys_names[i], systems[i]) for i in range(num_sys)]\n",
    "\n",
    "ps = PairedTest(named_systems, metrics, references=refs,\n",
    "                test_type=test_type, n_samples=n_samples,\n",
    "                n_jobs=paired_jobs)\n",
    "\n",
    "# Set back the number of trials\n",
    "paired_n = ps.n_samples\n",
    "\n",
    "# Run the test\n",
    "sigs, scores = ps()\n",
    "\n",
    "# Get signature strings\n",
    "sigs = {k: v.format(short) for k, v in sigs.items()}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hujghFHRuqLy"
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(scores['System'][index])\n",
    "print(\"BLEU: \", scores['BLEU'][index])\n",
    "print(\"ROUGE: \", scores['Rouge'][index])\n",
    "print(\"SARI: \", scores['SARI'][index])\n",
    "print(\"FRE: \", scores['FRE'][index])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
