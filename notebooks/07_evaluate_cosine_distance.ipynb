{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6JahOwvN6O5Y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2UxczOXMUf9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#download the data\n",
        "!git clone https://dev:dtKN5sX9We7pw1soPB19@gitlab.lrz.de/josh-o/leichte-sprache-corpus.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#install dependencies (April 14, 2023)\n",
        "#pytorch 2.0.0+cu118\n",
        "#Python 3.9.16\n",
        "!pip install transformers==4.28.0 \n",
        "!pip install sentencepiece==0.1.98\n",
        "!pip install datasets==2.11.0"
      ],
      "metadata": {
        "id": "_m1jIpLqMbeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "PREFIX = \"/content/leichte-sprache-corpus/aligned/20min/\"\n",
        "PREFIX_KURIER = \"/content/leichte-sprache-corpus/aligned/kurier/\"\n",
        "PREFIX_AUGMENTED = \"/content/leichte-sprache-corpus/aligned/20min/augmented/\"\n",
        "PREFIX_AUGMENTED_KURIER = \"/content/leichte-sprache-corpus/aligned/kurier/augmented/\"\n",
        "\n",
        "model_path, revision = (\"facebook/mbart-large-cc25\", \"57cecec5a3185d3ec7b3021a53093cf96835a634\")"
      ],
      "metadata": {
        "id": "K3E3QL_TMf47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#experiments\n",
        "\n",
        "#20min dropout tests\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX + \"20min_aligned_train.csv\", None, 0.1, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX + \"20min_aligned_train.csv\", None, 0.3, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX + \"20min_aligned_train.csv\", None, 0.8, 0.0)\n",
        "\n",
        "#20in data augmentation test\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX + \"20min_aligned_train.csv\" ,PREFIX_AUGMENTED + \"simple_noise.csv\", 0.0, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX + \"20min_aligned_train.csv\", PREFIX_AUGMENTED + \"bart_noise.csv\", 0.0, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX + \"20min_aligned_train.csv\", PREFIX_AUGMENTED + \"inputs_back_google.csv\", 0.0, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX + \"20min_aligned_train.csv\", PREFIX_AUGMENTED + \"inputs_back_google_simple_noise.csv\", 0.0, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX + \"20min_aligned_train.csv\", PREFIX_AUGMENTED + \"inputs_english_deepl.csv\", 0.0, 0.0)\n",
        "\n",
        "#Kurier dropout tests\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX_KURIER + \"kurier_aligned_train.csv\", None, 0.1, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX_KURIER + \"kurier_aligned_train.csv\", None, 0.3, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX_KURIER + \"kurier_aligned_train.csv\", None, 0.8, 0.0)\n",
        "\n",
        "#Kurier data augmentation test\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX_KURIER + \"kurier_aligned_train.csv\" ,PREFIX_AUGMENTED_KURIER + \"simple_noise.csv\", 0.0, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX_KURIER + \"kurier_aligned_train.csv\", PREFIX_AUGMENTED_KURIER + \"bart_noise.csv\", 0.0, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX_KURIER + \"kurier_aligned_train.csv\", PREFIX_AUGMENTED_KURIER + \"inputs_back_google.csv\", 0.0, 0.0)\n",
        "#data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX_KURIER + \"kurier_aligned_train.csv\", PREFIX_AUGMENTED_KURIER + \"inputs_back_google_simple_noise.csv\", 0.0, 0.0)\n",
        "data, augmentation_data, encoder_dropout, decoder_dropout = (PREFIX_KURIER + \"kurier_aligned_train.csv\", PREFIX_AUGMENTED_KURIER + \"inputs_english_deepl.csv\", 0.0, 0.0)"
      ],
      "metadata": {
        "id": "KgKEFbzT4xIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "TUWbETR64meh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartForConditionalGeneration, MBartTokenizerFast, MBartConfig\n",
        "\n",
        "model_config = MBartConfig.from_pretrained(model_path)\n",
        "model_config.dropout = 0.0\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_path, config=model_config, revision=revision)\n",
        "\n",
        "tokenizer = MBartTokenizerFast.from_pretrained(model_path, src_lang=\"de_DE\", tgt_lang=\"de_DE\")\n",
        "\n",
        "# set decoding params\n",
        "model.config.decoder_start_token_id=250003\n",
        "model.config.forced_bos_token_id=0\n",
        "model.config.max_length = 1024\n",
        "\n",
        "#freeze all\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#make cross attention trainable\n",
        "for layer in model.model.decoder.layers:\n",
        "  for param in layer.encoder_attn.parameters():\n",
        "    param.requires_grad = True\n",
        "  for param in layer.encoder_attn_layer_norm.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "#unfreeze batchnorms\n",
        "for module in model.modules():\n",
        "  if isinstance(module, torch.nn.LayerNorm):\n",
        "    for param in module.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "def set_dropout(model, p_encoder, p_decoder):\n",
        "  \n",
        "  model.model.encoder.dropout = p_encoder\n",
        "  for layer in model.model.encoder.layers:\n",
        "    layer.dropout = p_encoder\n",
        "\n",
        "  model.model.decoder.dropout = p_decoder\n",
        "  for layer in model.model.decoder.layers:\n",
        "    layer.dropout = p_decoder\n",
        "\n",
        "set_dropout(model, p_encoder=encoder_dropout, p_decoder=decoder_dropout)"
      ],
      "metadata": {
        "id": "HJW--i5LMm0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "91TT63yK4pX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset,concatenate_datasets, Features, Value\n",
        "import unicodedata\n",
        "\n",
        "def normalize(text):\n",
        "  text['normal_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['normal_phrase'].strip())\n",
        "  text['simple_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['simple_phrase'].strip())\n",
        "  return text\n",
        "\n",
        "def tokenize(text, input_tokenizer, output_tokenizer, max_input_length):\n",
        "  inputs = input_tokenizer(text[\"normal_phrase\"], return_tensors=\"np\")\n",
        "  labels = output_tokenizer(text[\"simple_phrase\"], return_tensors=\"np\", truncation=True, max_length=max_input_length)\n",
        "  inputs['labels'] = labels['input_ids']\n",
        "  return inputs\n",
        "\n",
        "def count(text):\n",
        "  text['length'] = len(text['input_ids']) * len(text['labels'])\n",
        "  return text\n",
        "\n",
        "def get_dataset(data_files, input_tokenizer, output_tokenizer, name=None, augmentation_file=None,max_input_length=None, augmentation_tokenizer=None):\n",
        "  features = Features({'normal_phrase': Value('string'), 'simple_phrase': Value('string')})\n",
        "\n",
        "  data = load_dataset(\"csv\",name=name, data_files=data_files, features=features)\n",
        "  data = data.map(normalize, num_proc=4)\n",
        "  data = data.map(lambda rows: tokenize(rows, input_tokenizer, output_tokenizer, max_input_length), batched=True)\n",
        "  if \"train\" in data:\n",
        "    data['train'] = data['train'].map(count, num_proc=4)\n",
        "    data = data.remove_columns([column for column in data.column_names['train'] if column not in ['labels','input_ids','attention_mask','length']])\n",
        "  else:\n",
        "    data = data.remove_columns([column for column in data.column_names['test'] if column not in ['labels','input_ids','attention_mask','length']])\n",
        "\n",
        "  if augmentation_file is not None:\n",
        "    if augmentation_tokenizer is None:\n",
        "      augmentation_tokenizer = input_tokenizer\n",
        "    #add augmented input to the train dataset if given\n",
        "    data_a = load_dataset(\"csv\", data_files=augmentation_file, features=features)\n",
        "    data_a = data_a.map(normalize, num_proc=4)\n",
        "    data_a = data_a.map(lambda row: augmentation_tokenizer(row[\"normal_phrase\"], return_tensors=\"np\", truncation=True, max_length=max_input_length), batched=True)\n",
        "    data_a = data_a.rename_column(\"input_ids\", \"augmented_ids\")\n",
        "    data_a = data_a.rename_column(\"attention_mask\", \"augmented_mask\")\n",
        "    data['train'] = concatenate_datasets([data['train'], data_a['train']], axis=1)\n",
        "    data['train'] = data['train'].remove_columns([column for column in data.column_names['train'] if column not in ['labels','input_ids','attention_mask','length','augmented_ids','augmented_mask']])\n",
        "\n",
        "  if max_input_length is not None:\n",
        "    data = data.filter(lambda example: len(example[\"input_ids\"]) < max_input_length)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "FPtYcxpbMvlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train on 20min\n",
        "data_train = get_dataset({'train': data}, tokenizer, tokenizer, augmentation_file=augmentation_data, max_input_length=model.config.max_length)"
      ],
      "metadata": {
        "id": "0BjhVDBJ5nc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "lgWOrbKh4r_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "class CustomCollator(DataCollatorForSeq2Seq):\n",
        "    def __call__(self, features, return_tensors=None):\n",
        "\n",
        "        augmented_ids = [feature[\"augmented_ids\"] for feature in features] if \"augmented_ids\" in features[0].keys() else None\n",
        "        augmented_mask = [feature[\"augmented_mask\"] for feature in features] if \"augmented_mask\" in features[0].keys() else None\n",
        "\n",
        "        if augmented_ids is not None and augmented_mask is not None:\n",
        "            #process augmentation data\n",
        "            temp_features = []\n",
        "            for feature in features:\n",
        "                temp_feature = {}\n",
        "                temp_feature['input_ids'] = feature.pop(\"augmented_ids\")\n",
        "                temp_feature['attention_mask'] = feature.pop(\"augmented_mask\")\n",
        "                temp_features.append(temp_feature)\n",
        "\n",
        "            temp_features = self.__call__(temp_features)\n",
        "            for i, feature in enumerate(features):\n",
        "                feature[\"augmented_ids\"] = temp_features[\"input_ids\"][i].tolist()\n",
        "                feature[\"augmented_mask\"] = temp_features[\"attention_mask\"][i].tolist()\n",
        "\n",
        "        return super().__call__(features, return_tensors)\n",
        "\n",
        "data_collator = CustomCollator(tokenizer=tokenizer, model=model, pad_to_multiple_of=8)"
      ],
      "metadata": {
        "id": "ujd71ZfuXBBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "all_loss = []\n",
        "\n",
        "train_dataloader = DataLoader(data_train['train'], batch_size=2, collate_fn=data_collator)\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CosineSimilarity(dim=-1, eps=1e-08)\n",
        "\n",
        "with torch.no_grad():\n",
        "  \n",
        "  for item in tqdm(train_dataloader):\n",
        "\n",
        "    test_item = {}\n",
        "    test_item['input_ids'] = item['input_ids'].to(device)\n",
        "    test_item['attention_mask'] = item['attention_mask'].to(device)\n",
        "    test_item['labels'] = item['labels'].to(device)\n",
        "\n",
        "    augmented_item =test_item\n",
        "    if augmentation_data is not None:\n",
        "      augmented_item = {}\n",
        "      augmented_item['input_ids'] = item['augmented_ids'].to(device)\n",
        "      augmented_item['attention_mask'] = item['augmented_mask'].to(device)\n",
        "      augmented_item['labels'] = item['labels'].to(device)\n",
        "\n",
        "    model.eval()\n",
        "    outputs_original = model(**test_item, output_hidden_states=True)\n",
        "    if augmentation_data is None:\n",
        "      model.train()\n",
        "    outputs_augmented  = model(**augmented_item, output_hidden_states=True)\n",
        "\n",
        "    mask = test_item['labels'] != -100\n",
        "    mask = mask\n",
        "\n",
        "    loss = loss_fn(outputs_original.decoder_hidden_states[-1], outputs_augmented.decoder_hidden_states[-1])\n",
        "\n",
        "    loss = torch.sum(loss * mask, dim=-1) / torch.sum(mask, dim=-1)\n",
        "\n",
        "    all_loss.extend(loss.detach().tolist())\n",
        "\n",
        "losses = torch.tensor(all_loss + all_loss)\n",
        "print(\"Mean: \", losses.mean())\n",
        "print(\"STD: \", losses.std())"
      ],
      "metadata": {
        "id": "86jL_EqcM2yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Auto Disconnect from Colab to Save Credits"
      ],
      "metadata": {
        "id": "6JahOwvN6O5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "r8E-xip56mg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}