{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4Fnybydk3U5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#download the data\n",
    "!git clone https://dev:dtKN5sX9We7pw1soPB19@gitlab.lrz.de/josh-o/leichte-sprache-corpus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwoHk2v6B7q8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#install dependencies (April 14, 2023)\n",
    "#pytorch 2.0.0+cu118\n",
    "#Python 3.9.16\n",
    "!pip install transformers==4.28.0 \n",
    "!pip install sentencepiece==0.1.98\n",
    "!pip install datasets==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yih9i6yHqhgT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import set_seed\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "PREFIX = \"../../leichte-sprache-corpus/monolingual/bart_noise/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YwirQ1-EmRD"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9GDKUzDE1Ls"
   },
   "outputs": [],
   "source": [
    "#choose your desired input mode\n",
    "\n",
    "NO_INPUT = \"no_input\"\n",
    "GAUSSIAN_NOISE = \"gaussian_noise_input\"\n",
    "BART_NOISE = \"bart_noise_input\"\n",
    "\n",
    "input_mode = GAUSSIAN_NOISE#BART_NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CUoRaeOB4xE"
   },
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer, MBartForCausalLM\n",
    "\n",
    "model_path = \"facebook/mbart-large-cc25\"\n",
    "model_type = MBartForCausalLM\n",
    "if input_mode == BART_NOISE:\n",
    "  model_type = MBartForConditionalGeneration\n",
    "\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_path, tgt_lang = \"de_DE\", src_lang = \"de_DE\")\n",
    "\n",
    "model = model_type.from_pretrained(model_path)\n",
    "\n",
    "#freeze all\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "#unfreeze self attention\n",
    "for layer in model.model.decoder.layers:\n",
    "  for param in layer.self_attn.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#unfreeze batchnorms\n",
    "for module in model.modules():\n",
    "  if isinstance(module, torch.nn.LayerNorm):\n",
    "    for param in module.parameters():\n",
    "      param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDrJ0ZX3EiKy"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSUyx8AvEP_V"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,concatenate_datasets, Features, Value\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def normalize(text):\n",
    "  text['normal_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['normal_phrase'].strip())\n",
    "  text['simple_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['simple_phrase'].strip())\n",
    "  return text\n",
    "\n",
    "def tokenize(text, tokenizer, max_input_length):\n",
    "  inputs = tokenizer(text[\"normal_phrase\"], return_tensors=\"np\")\n",
    "  labels = tokenizer(text[\"simple_phrase\"], return_tensors=\"np\", truncation=True, max_length=max_input_length)\n",
    "  inputs['labels'] = labels['input_ids']\n",
    "  return inputs\n",
    "\n",
    "def count(text):\n",
    "  #calculate the length token which is used to group the data samples\n",
    "  #(we want to have the data sample with the highest memory consumption at the first place to force early Out-Of-Memory issues)\n",
    "  text['length'] = len(text['labels'])\n",
    "  return text\n",
    "\n",
    "def add_gaussian_noise(row):\n",
    "  sequence_length = random.randint(128, 1024)\n",
    "  row['random_sequence_length'] = sequence_length\n",
    "  return row\n",
    "\n",
    "def add_decoder_atention_mask(row):\n",
    "  row['attention_mask'] = [1]*len(row['input_ids'])\n",
    "  return row\n",
    "\n",
    "def get_dataset(data_files, tokenizer, input_mode, name=None, max_input_length=None):\n",
    "  features = Features({'normal_phrase': Value('string'), 'simple_phrase': Value('string')})\n",
    "\n",
    "  data = load_dataset(\"csv\",name=name, data_files=data_files, features=features)['train'].train_test_split(test_size=0.1)\n",
    "  data = data.map(normalize, num_proc=4)\n",
    "  data = data.map(lambda rows: tokenize(rows, tokenizer, max_input_length), batched=True)\n",
    "\n",
    "  data['train'] = data['train'].map(count, num_proc=4)\n",
    "  data = data.remove_columns([column for column in data.column_names['train'] if column not in ['labels','input_ids','attention_mask','length', 'random_sequence_length']])\n",
    "  if max_input_length is not None:\n",
    "    data = data.filter(lambda example: len(example[\"input_ids\"]) < max_input_length)\n",
    "\n",
    "  if input_mode == GAUSSIAN_NOISE:\n",
    "    data = data.map(lambda rows: add_gaussian_noise(rows))\n",
    "\n",
    "  if input_mode != BART_NOISE:\n",
    "    #decoder only\n",
    "    data = data.remove_columns([column for column in data.column_names['train'] if column not in ['labels','length', 'random_sequence_length']])\n",
    "    data = data.rename_column(\"labels\", \"input_ids\")\n",
    "    data = data.map(lambda rows: add_decoder_atention_mask(rows))\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTbEOmUOF2EM"
   },
   "outputs": [],
   "source": [
    "data_files_monolingual = [PREFIX + \"nachrichtenleicht_noise.csv\",\n",
    "                          PREFIX + \"ndr_noise.csv\",\n",
    "                          PREFIX + \"einfachstars_noise.csv\",\n",
    "                          PREFIX + \"hda_sprachtechnologie_noise.csv\",\n",
    "                          PREFIX + \"lebenshilfe_noise.csv\",\n",
    "                          PREFIX + \"hurraki_noise.csv\",\n",
    "                          PREFIX + \"kurier_noise.csv\"]\n",
    "\n",
    "data_train = get_dataset(data_files_monolingual, tokenizer, input_mode, \"20min\", max_input_length=model.config.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "civdFElLNjSP"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdPpJ63tzB8j"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "import numpy as np\n",
    "\n",
    "data_collator_type = DataCollatorForLanguageModeling\n",
    "data_collator_args = {'mlm':False, 'pad_to_multiple_of':8}\n",
    "if input_mode == BART_NOISE:\n",
    "  data_collator_type = DataCollatorForSeq2Seq\n",
    "  data_collator_args = {'model':model, 'pad_to_multiple_of':8}\n",
    "\n",
    "class CustomCollator(data_collator_type):\n",
    "  def __call__(self, features, return_tensors=None):\n",
    "\n",
    "    encoder_hidden_states = [torch.randn(feature.pop(\"random_sequence_length\"), 1024) for feature in features] if \"random_sequence_length\" in features[0].keys() else None\n",
    "    if encoder_hidden_states:\n",
    "      for i, feature in enumerate(features):\n",
    "        feature['encoder_hidden_states'] = encoder_hidden_states[i].numpy()\n",
    "    \n",
    "    for feature in features:\n",
    "      feature.pop('length',None)\n",
    "    \n",
    "\n",
    "    if encoder_hidden_states is not None:\n",
    "      #align width\n",
    "      max_width = max(len(a[:,0]) for a in encoder_hidden_states)\n",
    "      if self.pad_to_multiple_of is not None:\n",
    "          max_width = (\n",
    "              (max_width + self.pad_to_multiple_of - 1)\n",
    "              // self.pad_to_multiple_of\n",
    "              * self.pad_to_multiple_of\n",
    "          )\n",
    "\n",
    "      padding_side = self.tokenizer.padding_side\n",
    "      for feature in features:\n",
    "          remainder = [0] *  1024 \n",
    "          remainder = [remainder] * (max_width - len(feature[\"encoder_hidden_states\"][:,0]))\n",
    "          if (max_width - len(feature[\"encoder_hidden_states\"][:,0])) == 0:\n",
    "            continue\n",
    "          if padding_side == \"right\":\n",
    "              feature[\"encoder_hidden_states\"] = np.concatenate([feature[\"encoder_hidden_states\"], remainder], axis=0).astype(np.float32)\n",
    "          else:\n",
    "              feature[\"encoder_hidden_states\"] = np.concatenate([remainder, feature[\"encoder_hidden_states\"]], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "    return super().__call__(features, return_tensors)\n",
    "\n",
    "data_collator = CustomCollator(tokenizer=tokenizer, **data_collator_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqGgUEp0f2Jr"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_arguments, trainer = TrainingArguments, Trainer\n",
    "if input_mode == BART_NOISE:\n",
    "  training_arguments, trainer = Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "#finetune for one epoch on all data\n",
    "training_args = training_arguments(\n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=2, \n",
    "    gradient_accumulation_steps=16,\n",
    "    warmup_steps=200,\n",
    "    logging_steps=20,\n",
    "    fp16=True,\n",
    "    label_smoothing_factor=0.1 ,\n",
    "    group_by_length=True,\n",
    "    seed=seed,\n",
    "    data_seed=seed,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    optim='adamw_torch',\n",
    ")\n",
    "\n",
    "trainer = trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train['train'],\n",
    "    eval_dataset=data_train['test'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAwR40p2FJ6V"
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPg1VC8KIl_3"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mF1gyQEIBJa"
   },
   "outputs": [],
   "source": [
    "#convert to causal lm\n",
    "model.save_pretrained(\"temp\")\n",
    "model = MBartForCausalLM.from_pretrained(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2TOwNjXUW58"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"mbart-decoder-easy\", commit_message=\"Trained with cross-attention (gaussian noise)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrsUEKj3ITND"
   },
   "source": [
    "## Auto Disconnect from Colab to Save Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-95My6VuIeFk"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DrsUEKj3ITND"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
