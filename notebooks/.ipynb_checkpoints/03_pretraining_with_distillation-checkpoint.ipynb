{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4CJFDpoTJaT"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#download the data\n",
    "!git clone https://dev:dtKN5sX9We7pw1soPB19@gitlab.lrz.de/josh-o/leichte-sprache-corpus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37M8rUt4TW4r"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#install dependencies (April 14, 2023)\n",
    "#pytorch 2.0.0+cu118\n",
    "#Python 3.9.16\n",
    "!pip install transformers==4.28.0 \n",
    "!pip install sentencepiece==0.1.98\n",
    "!pip install pytokenizations==0.8.4\n",
    "!pip install datasets==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46blZRvnTZaL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import EncoderDecoderModel, AutoModelForCausalLM\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizerFast, MBartConfig\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import set_seed\n",
    "\n",
    "import random\n",
    "import tokenizations\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed) # no direct effect on text generation\n",
    "\n",
    "PREFIX = \"../../leichte-sprache-corpus/aligned/20min/\"\n",
    "PREFIX_AUGMENTED = \"../../leichte-sprache-corpus/aligned/20min/augmented/\"\n",
    "\n",
    "encoder_path = \"facebook/mbart-large-cc25\"\n",
    "decoder_path = \"josh-oo/german-gpt2-easy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp7dd4lxLBXV"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YccWendbnLCX"
   },
   "source": [
    "Choose the model you want to train be uncommenting it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0orM464jnYIx"
   },
   "source": [
    "## mBART with Custom GPT-2 decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yUWtWxiTlAY"
   },
   "outputs": [],
   "source": [
    "#mBART with gpt-2 decoder:\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "#modified GPT2Block to access the cross attentions keys and values\n",
    "#copied from huggingface transformers modeling_gpt2.py\n",
    "class CachableGPT2Block(GPT2Block):\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "        outputs = attn_outputs[1:]\n",
    "        # residual connection\n",
    "        hidden_states = attn_output + residual\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            # add one self-attention block for cross-attention\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
    "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "            residual = hidden_states\n",
    "            hidden_states = self.ln_cross_attn(hidden_states)\n",
    "            cross_attn_outputs = self.crossattention(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "            )\n",
    "            attn_output = cross_attn_outputs[0]\n",
    "            # residual connection\n",
    "            hidden_states = residual + attn_output\n",
    "            outputs = list(outputs)\n",
    "            outputs[0] = (outputs[0][0],outputs[0][1]) + cross_attn_outputs[1]\n",
    "            outputs = tuple(outputs)\n",
    "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        # residual connection\n",
    "        hidden_states = residual + feed_forward_hidden_states\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "        return outputs  # hidden_states, present, (attentions, cross_attentions)\n",
    "\n",
    "\n",
    "class DummyEncoder(torch.nn.Module):\n",
    "  def __init__(self, config):\n",
    "    self.config = config\n",
    "    super().__init__()\n",
    "\n",
    "#prepare output_tokenizer\n",
    "\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "  outputs = token_ids_0 + [self.eos_token_id]\n",
    "  return outputs\n",
    "\n",
    "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "\n",
    "output_tokenizer = GPT2Tokenizer.from_pretrained(decoder_path)\n",
    "\n",
    "output_tokenizer.pad_token_id = 1\n",
    "output_tokenizer.bos_token_id = 0\n",
    "output_tokenizer.eos_token_id = 2\n",
    "\n",
    "#output_tokenizers = (output_tokenizer, output_tokenizer_fast)\n",
    "\n",
    "encoder_config = MBartConfig.from_pretrained(encoder_path)\n",
    "\n",
    "mbart = MBartForConditionalGeneration.from_pretrained(encoder_path, config=encoder_config)\n",
    "\n",
    "input_tokenizer = MBartTokenizerFast.from_pretrained(encoder_path)\n",
    "if hasattr(input_tokenizer, \"src_lang\"):\n",
    "  input_tokenizer.src_lang = \"de_DE\"\n",
    "\n",
    "decoder = AutoModelForCausalLM.from_pretrained(decoder_path)\n",
    "#start of fix: cross attention value output\n",
    "decoder.save_pretrained(\"tmp\")\n",
    "for i in range(0, len(decoder.transformer.h)):\n",
    "  decoder.transformer.h[i] = CachableGPT2Block(config=decoder.config, layer_idx=i)\n",
    "decoder.from_pretrained(\"tmp\")\n",
    "\n",
    "model = EncoderDecoderModel(encoder=mbart.model.encoder,decoder=decoder)\n",
    "\n",
    "encoder_model = mbart.model.encoder\n",
    "teacher = mbart.model.decoder\n",
    "\n",
    "model.encoder = DummyEncoder(model.encoder.config)\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id = output_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = output_tokenizer.eos_token_id\n",
    "model.config.pad_token_id = 1\n",
    "model.config.max_length = 1024\n",
    "\n",
    "#freeze all\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#make cross attention trainable\n",
    "for module in model.decoder.transformer.h:\n",
    "  for param in module.crossattention.parameters():\n",
    "    param.requires_grad = True\n",
    "  for param in module.ln_cross_attn.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "if hasattr(model,'enc_to_dec_proj'):\n",
    "  model.enc_to_dec_proj.requires_grad = True\n",
    "\n",
    "#unfreeze batchnorms\n",
    "for module in model.modules():\n",
    "  if isinstance(module, torch.nn.LayerNorm):\n",
    "    for param in module.parameters():\n",
    "      param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9hxuw9Bne24"
   },
   "source": [
    "## mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4ooa2zwh0xW"
   },
   "outputs": [],
   "source": [
    "#pure mBART:\n",
    "\"\"\"\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer, MBartConfig\n",
    "import copy\n",
    "\n",
    "model_config = MBartConfig.from_pretrained(encoder_path)\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(encoder_path, config=model_config)\n",
    "\n",
    "teacher = copy.deepcopy(model.model.decoder)\n",
    "encoder_model = model.model.encoder\n",
    "model.model.encoder = None\n",
    "\n",
    "input_tokenizer = MBartTokenizer.from_pretrained(encoder_path)\n",
    "if hasattr(input_tokenizer, \"src_lang\"):\n",
    "  input_tokenizer.src_lang = \"de_DE\"\n",
    "\n",
    "output_tokenizer = MBartTokenizer.from_pretrained(encoder_path)\n",
    "if hasattr(output_tokenizer, \"src_lang\"):\n",
    "  output_tokenizer.src_lang = \"de_DE\"\n",
    "\n",
    "# set decoding params\n",
    "model.config.decoder_start_token_id=250003\n",
    "model.config.eos_token_id = output_tokenizer.eos_token_id\n",
    "model.config.pad_token_id = 1\n",
    "model.config.max_length = 1024\n",
    "\n",
    "#freeze all\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#make cross attention trainable\n",
    "for layer in model.model.decoder.layers:\n",
    "  for param in layer.encoder_attn.parameters():\n",
    "    param.requires_grad = True\n",
    "  for param in layer.encoder_attn_layer_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#unfreeze batchnorms\n",
    "for module in model.modules():\n",
    "  if isinstance(module, torch.nn.LayerNorm):\n",
    "    for param in module.parameters():\n",
    "      param.requires_grad = True\n",
    "\n",
    "#reset cross-attention to random parameters\n",
    "for layer in model.model.decoder.layers:\n",
    "    layer.encoder_attn.apply(model._init_weights)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onQ5eOK1LIgB"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8l4logOLXeI"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,concatenate_datasets, Features, Value\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "  text['normal_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['normal_phrase'].strip())\n",
    "  text['simple_phrase'] = \"<s>\" + unicodedata.normalize(\"NFC\",text['simple_phrase'].strip())\n",
    "  return text\n",
    "\n",
    "def tokenize(text, input_tokenizer, output_tokenizer, max_input_length):\n",
    "  inputs = input_tokenizer(text[\"normal_phrase\"], return_tensors=\"np\")\n",
    "  labels = output_tokenizer(text[\"simple_phrase\"], return_tensors=\"np\", truncation=True, max_length=max_input_length)\n",
    "  inputs['labels'] = labels['input_ids']\n",
    "  return inputs\n",
    "\n",
    "def count(text):\n",
    "  #calculate the length token which is used to group the data samples\n",
    "  #we use len(input) * len(output) as it models the maximum GPU memory consumption the best\n",
    "  #(we want to have the data sample with the highest memory consumption at the first place to force early Out-Of-Memory issues)\n",
    "  text['length'] = len(text['input_ids']) * len(text['labels'])\n",
    "  return text\n",
    "\n",
    "## knowledge distillation stuff:\n",
    "\n",
    "def teacher_decoder_inputs(text, teacher_tokenizer, max_input_length):\n",
    "  decoder_inputs = input_tokenizer(text[\"simple_phrase\"], return_tensors=\"np\", truncation=True, max_length=max_input_length)\n",
    "  text['teacher_decoder_input_ids'] = decoder_inputs['input_ids']\n",
    "  return text\n",
    "\n",
    "def compute_token_transform(input_tokens, output_tokens, interpolate=False):\n",
    "  in2out, out2in = tokenizations.get_alignments(input_tokens, output_tokens)\n",
    "\n",
    "  vector = torch.zeros(len(output_tokens), len(input_tokens))\n",
    "\n",
    "  for i, mapping in enumerate(in2out):\n",
    "      vector[mapping,i] = 1\n",
    "  \n",
    "  if interpolate == False:\n",
    "    #interpolate rows with no direct token connection (please refer master thesis Figure 4.2 lower branch)\n",
    "    vector = torch.where(vector > 0.99, vector, 0.0)\n",
    "\n",
    "  vector = (vector.T / vector.sum(dim=1))\n",
    "  #division by zero leads to nan values \n",
    "  if torch.isnan(vector).any():\n",
    "    vector = torch.nan_to_num(vector, nan=0.0)\n",
    "  return vector.T\n",
    "\n",
    "def compute_all_token_transforms(row, teacher_tokenizer, student_tokenizer, max_input_length=None):\n",
    "  simple_text = row[\"simple_phrase\"]\n",
    "  input_tokens = [\"<s>\"] + teacher_tokenizer.tokenize(simple_text.strip())\n",
    "  output_tokens = [\"<s>\"] + student_tokenizer.tokenize(simple_text.strip())\n",
    "\n",
    "  input_tokens =  [\"#\" + teacher_tokenizer.convert_tokens_to_string([t]).strip() for t in input_tokens]\n",
    "  output_tokens = [\"#\" + student_tokenizer.convert_tokens_to_string([t]).strip() for t in output_tokens]\n",
    "\n",
    "  transform = compute_token_transform(input_tokens, output_tokens)\n",
    "  if max_input_length is not None:\n",
    "    transform = transform[:max_input_length,:max_input_length]\n",
    "  #convert sparse tensor to make it serializable as the datasets library requires cachable items\n",
    "  transform = transform.to_sparse()\n",
    "  row['token_transform_size'] = transform.size()\n",
    "  row['token_transform_values'] = transform.values()\n",
    "  row['token_transform_indices'] = transform.indices()\n",
    "  return row\n",
    "\n",
    "def get_dataset(data_files, input_tokenizer, output_tokenizer, name=None, max_input_length=None):\n",
    "  features = Features({'normal_phrase': Value('string'), 'simple_phrase': Value('string')})\n",
    "\n",
    "  data = load_dataset(\"csv\",name=name, data_files=data_files, features=features)\n",
    "  data = data.map(normalize, num_proc=4)\n",
    "  data = data.map(lambda rows: tokenize(rows, input_tokenizer, output_tokenizer, max_input_length), batched=True)\n",
    "\n",
    "  if type(input_tokenizer) is not type(output_tokenizer):\n",
    "    data['train'] = data['train'].map(lambda rows: teacher_decoder_inputs(rows, output_tokenizer, max_input_length), batched=True)\n",
    "    #disable caching as sparse tensors can't be cached and saving dense tensors is to expensive\n",
    "    data['train'] = data['train'].map(lambda rows: compute_all_token_transforms(rows, input_tokenizer, output_tokenizer,max_input_length), num_proc=4)\n",
    "\n",
    "\n",
    "  if \"train\" in data:\n",
    "    data['train'] = data['train'].map(count, num_proc=4)\n",
    "    data = data.remove_columns([column for column in data.column_names['train'] if column not in ['labels','input_ids','attention_mask','length', 'teacher_decoder_input_ids', 'token_transform_size', 'token_transform_values', 'token_transform_indices']])\n",
    "  else:\n",
    "    data = data.remove_columns([column for column in data.column_names['test'] if column not in ['labels','input_ids','attention_mask','length']])\n",
    "\n",
    "  if max_input_length is not None:\n",
    "    data = data.filter(lambda example: len(example[\"input_ids\"]) < max_input_length)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHS8g4XMMBII"
   },
   "outputs": [],
   "source": [
    "#choose the right dataset to pre-train:\n",
    "\n",
    "data_files_20_min = {'train': PREFIX_AUGMENTED + \"pretraining.csv\", 'val': PREFIX + \"20min_aligned_dev.csv\"}\n",
    "#data_files_20_min = {'train': PREFIX + \"20min_aligned_train.csv\", 'val': PREFIX + \"20min_aligned_dev.csv\"}\n",
    "\n",
    "data_train = get_dataset(data_files_20_min, input_tokenizer, output_tokenizer, \"20min\", max_input_length=model.config.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SAOIGMdMM3J"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyhUarhLWA9x"
   },
   "outputs": [],
   "source": [
    "class CustomCollator(DataCollatorForSeq2Seq):\n",
    "  def __call__(self, features, return_tensors=None):\n",
    "\n",
    "    transform_i = [feature.pop(\"token_transform_indices\") for feature in features] if \"token_transform_indices\" in features[0].keys() else None\n",
    "    transform_v = [feature.pop(\"token_transform_values\") for feature in features] if \"token_transform_values\" in features[0].keys() else None\n",
    "    transform_s = [feature.pop(\"token_transform_size\") for feature in features] if \"token_transform_size\" in features[0].keys() else None\n",
    "    \n",
    "    transforms = [torch.sparse_coo_tensor(i, v, s).to_dense() for i,v,s in zip(transform_i, transform_v, transform_s)] if transform_i else None\n",
    "    if transforms:\n",
    "      for i, feature in enumerate(features):\n",
    "        feature['token_transform'] = transforms[i]\n",
    "    teacher_decoder_input_ids = [feature[\"teacher_decoder_input_ids\"] for feature in features] if \"teacher_decoder_input_ids\" in features[0].keys() else None\n",
    "\n",
    "    max_label_length = 0\n",
    "    if teacher_decoder_input_ids is not None:\n",
    "      max_label_length = max(len(l) for l in teacher_decoder_input_ids)\n",
    "      if self.pad_to_multiple_of is not None:\n",
    "          max_label_length = (\n",
    "              (max_label_length + self.pad_to_multiple_of - 1)\n",
    "              // self.pad_to_multiple_of\n",
    "              * self.pad_to_multiple_of\n",
    "          )\n",
    "\n",
    "      padding_side = self.tokenizer.padding_side\n",
    "      for feature in features:\n",
    "          remainder = [self.tokenizer.pad_token_id] * (max_label_length - len(feature[\"teacher_decoder_input_ids\"]))\n",
    "          if isinstance(feature[\"teacher_decoder_input_ids\"], list):\n",
    "              feature[\"teacher_decoder_input_ids\"] = (\n",
    "                  feature[\"teacher_decoder_input_ids\"] + remainder if padding_side == \"right\" else remainder + feature[\"teacher_decoder_input_ids\"]\n",
    "              )\n",
    "          elif padding_side == \"right\":\n",
    "              feature[\"teacher_decoder_input_ids\"] = np.concatenate([feature[\"teacher_decoder_input_ids\"], remainder]).astype(np.int64)\n",
    "          else:\n",
    "              feature[\"teacher_decoder_input_ids\"] = np.concatenate([remainder, feature[\"teacher_decoder_input_ids\"]]).astype(np.int64)\n",
    "\n",
    "    if transforms is not None:\n",
    "      #align width\n",
    "\n",
    "      padding_side = self.tokenizer.padding_side\n",
    "      for feature in features:\n",
    "          remainder = [0] *  (max_label_length  - len(feature[\"token_transform\"][0,:]))\n",
    "          remainder = [remainder] * len(feature[\"token_transform\"][:,0])\n",
    "          if padding_side == \"right\":\n",
    "              feature[\"token_transform\"] = np.concatenate([feature[\"token_transform\"], remainder], axis=1).astype(np.float32)\n",
    "          else:\n",
    "              feature[\"token_transform\"] = np.concatenate([remainder, feature[\"token_transform\"]], axis=1).astype(np.float32)\n",
    "\n",
    "      #align height\n",
    "      max_height = max(len(a[:,0]) for a in transforms)\n",
    "      if self.pad_to_multiple_of is not None:\n",
    "          max_height = (\n",
    "              (max_height + self.pad_to_multiple_of - 1)\n",
    "              // self.pad_to_multiple_of\n",
    "              * self.pad_to_multiple_of\n",
    "          )\n",
    "\n",
    "      padding_side = self.tokenizer.padding_side\n",
    "      for feature in features:\n",
    "          remainder = [0] * len(feature[\"token_transform\"][0,:])\n",
    "          remainder = [remainder] * (max_height - len(feature[\"token_transform\"][:,0]))\n",
    "          if padding_side == \"right\" and len(remainder) > 0:\n",
    "              feature[\"token_transform\"] = np.concatenate([feature[\"token_transform\"], remainder], axis=0).astype(np.float32)\n",
    "          elif len(remainder) > 0:\n",
    "              feature[\"token_transform\"] = np.concatenate([remainder, feature[\"token_transform\"]], axis=0).astype(np.float32)\n",
    "\n",
    "    return super().__call__(features, return_tensors)\n",
    "\n",
    "data_collator = CustomCollator(tokenizer=input_tokenizer, model=model, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLn-dXcLmsJu"
   },
   "outputs": [],
   "source": [
    "class ReduceAttnMap(torch.nn.Module):\n",
    "    #adapted from Attention Distillation: self-supervised vision transformer students need more guidance\n",
    "    def __init__(self, temperature, log_space=False):\n",
    "      super().__init__()\n",
    "      self.temperature = temperature\n",
    "      self.softmax = torch.nn.functional.softmax\n",
    "      if log_space:\n",
    "        self.softmax = torch.nn.functional.log_softmax\n",
    "\n",
    "    def forward(self, attn_map, attn_mask):\n",
    "      epsilon = 1e-6\n",
    "      attn_map = torch.add(attn_map, epsilon)\n",
    "      attn_map = torch.sum(torch.log(attn_map), dim=1)\n",
    "      attn_map = attn_map.masked_fill(~attn_mask, float('-inf'))\n",
    "      attn_map = self.softmax(self.temperature * attn_map, dim=-1)\n",
    "      attn_map = torch.nan_to_num(attn_map)\n",
    "\n",
    "      return attn_map\n",
    "\n",
    "class ReduceValueRelations(torch.nn.Module):\n",
    "    #adapted from MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers\n",
    "    def __init__(self, log_space=False, num_relation_heads=8):\n",
    "      super().__init__()\n",
    "      self.num_relation_heads = num_relation_heads\n",
    "      self.softmax = torch.nn.functional.softmax\n",
    "      if log_space:\n",
    "        self.softmax = torch.nn.functional.log_softmax\n",
    "\n",
    "    def forward(self, past_inputs, attn_mask):\n",
    "      epsilon = 1e-6\n",
    "\n",
    "      input_shape = past_inputs.shape\n",
    "      num_heads = input_shape[1]\n",
    "      attn_head_size = input_shape[-1]\n",
    "\n",
    "      rel_head_size = num_heads*attn_head_size // self.num_relation_heads\n",
    "\n",
    "      #merge heads\n",
    "      past_inputs = past_inputs.permute(0, 2, 1, 3).contiguous()\n",
    "      new_shape = past_inputs.size()[:-2] + (num_heads * attn_head_size,)\n",
    "      past_inputs = past_inputs.view(new_shape)\n",
    "\n",
    "      #split heads\n",
    "      new_shape = past_inputs.size()[:-1] + (self.num_relation_heads,  rel_head_size)\n",
    "      past_inputs = past_inputs.view(new_shape)\n",
    "      past_inputs = past_inputs.permute(0, 2, 1, 3)\n",
    "\n",
    "      past_inputs = torch.matmul(past_inputs,past_inputs.transpose(-1,-2))\n",
    "      past_inputs = past_inputs.masked_fill(~attn_mask, float('-inf'))\n",
    "      past_inputs = self.softmax((rel_head_size ** -0.5) * past_inputs, dim=-1)\n",
    "      past_inputs = torch.nan_to_num(past_inputs)\n",
    "\n",
    "      return past_inputs\n",
    "\n",
    "class DistillationTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, attention_alpha=0.5, attention_temperature=1/16, teacher_model=None, encoder_model=None, **kwargs):\n",
    "      super().__init__(**kwargs)\n",
    "      \n",
    "      self.MAX_LAYERS_TO_DISTILL = 2\n",
    "      self.current_stages = []\n",
    "\n",
    "      self.kl_loss = torch.nn.KLDivLoss(reduction=\"none\")\n",
    "      self.reduce_guide_attn_map = ReduceAttnMap(temperature=1/16)\n",
    "      self.reduce_actual_attn_map = ReduceAttnMap(temperature=attention_temperature, log_space=True)\n",
    "      self.reduce_guide_value_relations = ReduceValueRelations()\n",
    "      self.reduce_actual_value_relations = ReduceValueRelations(log_space=True)\n",
    "\n",
    "      self.attention_alpha = attention_alpha\n",
    "      self.attention_temperature = attention_temperature\n",
    "      self.teacher_model = teacher_model\n",
    "      self.encoder_model = encoder_model\n",
    "\n",
    "\n",
    "      self._move_model_to_device(self.teacher_model,self.model.device)\n",
    "      self._move_model_to_device(self.encoder_model,self.model.device)\n",
    "      self.teacher_model.eval()\n",
    "\n",
    "      self.decoder_head_mask = torch.ones(12,16, device=self.model.device)\n",
    "      self.decoder_head_mask = self.decoder_head_mask.bool()\n",
    "\n",
    "      if hasattr(self.model, \"decoder\") and hasattr(self.model.decoder.config, \"vocab_size\"):\n",
    "         self.decoder_vocab_size = self.model.decoder.config.vocab_size\n",
    "      else:\n",
    "         self.decoder_vocab_size =  self.model.config.vocab_size\n",
    "\n",
    "    def evaluate(self,eval_dataset = None,ignore_keys = None,metric_key_prefix = \"eval\",**gen_kwargs):\n",
    "        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix, num_beams=1, do_sample=False)#, top_k=3, penalty_alpha=0.6)\n",
    "\n",
    "    def predict(self,test_dataset,ignore_keys = None,metric_key_prefix = \"test\",**gen_kwargs):\n",
    "      return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix, num_beams=3, do_sample=False)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop('labels')\n",
    "        input_ids = inputs.get('input_ids')\n",
    "        attention_mask = inputs.get('attention_mask')\n",
    "\n",
    "        inputs.pop('length', None)\n",
    "\n",
    "        token_transform = inputs.pop('token_transform', None)\n",
    "\n",
    "        decoder_input_ids = inputs.pop('decoder_input_ids')\n",
    "        decoder_attention_mask = labels != -100\n",
    "\n",
    "        teacher_decoder_input_ids = inputs.pop('teacher_decoder_input_ids', None)\n",
    "        if teacher_decoder_input_ids is None:\n",
    "          teacher_decoder_input_ids = decoder_input_ids\n",
    "          teacher_decoder_attention_mask = decoder_attention_mask\n",
    "        else:\n",
    "          teacher_decoder_attention_mask = teacher_decoder_input_ids != 1\n",
    "          \n",
    "        # forward computation\n",
    "        with torch.no_grad():\n",
    "          encoder_hidden_states = self.encoder_model(**inputs, output_hidden_states=False, output_attentions=False)\n",
    "        outputs = model(**inputs,decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, encoder_outputs=encoder_hidden_states, output_attentions=True, use_cache=True)#, cross_attn_head_mask=self.decoder_head_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        regular_loss = outputs.loss\n",
    "        if self.label_smoother is not None:\n",
    "          regular_loss = self.label_smoother(outputs, labels, shift_labels=False)\n",
    "        else:\n",
    "          loss_fct = torch.nn.CrossEntropyLoss()\n",
    "          regular_loss = loss_fct(logits.view(-1, self.decoder_vocab_size), labels.view(-1))\n",
    "\n",
    "        total_attn_loss = torch.tensor(0.0, device=self.model.device)\n",
    "        if self.teacher_model is not None:\n",
    "\n",
    "          teacher_outputs = None\n",
    "          with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(input_ids=teacher_decoder_input_ids, attention_mask=teacher_decoder_attention_mask, encoder_hidden_states=encoder_hidden_states[0], encoder_attention_mask=attention_mask, output_attentions=True, use_cache=True)\n",
    "          \n",
    "            del encoder_hidden_states\n",
    "\n",
    "            attn_mask = torch.bmm(decoder_attention_mask.float().unsqueeze(2), attention_mask.float().unsqueeze(1)).bool()\n",
    "            vr_attn_mask = torch.bmm(attention_mask.float().unsqueeze(2), attention_mask.float().unsqueeze(1)).bool()\n",
    "            teacher_attn_mask = torch.bmm(teacher_decoder_attention_mask.float().unsqueeze(2), attention_mask.float().unsqueeze(1)).bool()\n",
    "\n",
    "            vr_attn_mask = vr_attn_mask.unsqueeze(1).repeat(1, 8, 1, 1)\n",
    "\n",
    "          #attention loss\n",
    "\n",
    "          current_progress = self.state.global_step / (self.state.max_steps +1)\n",
    "          current_stage = int(current_progress * (len(teacher_outputs['cross_attentions']) -  (self.MAX_LAYERS_TO_DISTILL - 1) - 1))\n",
    "          current_stages = [ current_stage + i for i in range(0, self.MAX_LAYERS_TO_DISTILL)]\n",
    "\n",
    "          #distill last layer and two additional layers randomly selected at each training step\n",
    "          current_stages = random.sample(range(0, len(teacher_outputs['cross_attentions']) - 1), 2) + [-1]\n",
    "          if current_stages != self.current_stages:\n",
    "            self.current_stages = current_stages\n",
    "            #print(\"Train new layers: \", current_stages)\n",
    "\n",
    "          for layer_index in current_stages:\n",
    "            with torch.no_grad():\n",
    "\n",
    "              #Guide attention distribution\n",
    "              guide_attn_map = teacher_outputs['cross_attentions'][layer_index]\n",
    "              guide_attention = self.reduce_guide_attn_map(guide_attn_map, teacher_attn_mask)\n",
    "\n",
    "              #Guide value relations\n",
    "              guide_key_values = teacher_outputs['past_key_values'][layer_index]\n",
    "              \n",
    "              guide_cross_attn_past_values = guide_key_values[-2:][1]\n",
    "              guide_vr = self.reduce_guide_value_relations(guide_cross_attn_past_values, vr_attn_mask)\n",
    "              \n",
    "              guide_cross_attn_past_keys = guide_key_values[-2:][0]\n",
    "              guide_kr = self.reduce_guide_value_relations(guide_cross_attn_past_keys, vr_attn_mask)\n",
    "\n",
    "\n",
    "              if token_transform is not None:\n",
    "                guide_attention = torch.matmul(token_transform, guide_attention)\n",
    "\n",
    "                no_match_mask = torch.unsqueeze(token_transform.sum(dim=-1), dim=-1)\n",
    "                no_match_mask = torch.repeat_interleave(no_match_mask, attn_mask.shape[-1], dim=-1)\n",
    "                attn_mask = (attn_mask * no_match_mask).bool()\n",
    "\n",
    "            #Actual attention distribution\n",
    "            actual_attn_map = outputs['cross_attentions'][layer_index]\n",
    "            actual_attention = self.reduce_actual_attn_map(actual_attn_map, attn_mask)\n",
    "\n",
    "            attn_loss = self.kl_loss(actual_attention, guide_attention)\n",
    "            del actual_attention\n",
    "            attn_loss = (attn_loss * (attn_mask)).sum(dim=[1,2])\n",
    "            attn_loss = attn_loss / attn_mask.sum(dim=1)[:,0] #Divide loss by number of tokens\n",
    "            attn_loss = attn_loss.sum() / guide_attention.size(0) # to have batch_mean\n",
    "\n",
    "            actual_key_values = outputs['past_key_values'][layer_index]\n",
    "            #Actual value relation\n",
    "          \n",
    "            actual_cross_attn_past_values = actual_key_values[-2:][1]\n",
    "            actual_vr = self.reduce_actual_value_relations(actual_cross_attn_past_values, vr_attn_mask)\n",
    "\n",
    "            vr_loss = self.kl_loss(actual_vr, guide_vr)\n",
    "            del actual_vr\n",
    "            vr_loss = (vr_loss * vr_attn_mask).sum(dim=[1,2,3])\n",
    "            vr_loss = vr_loss / vr_attn_mask.sum(dim=2)[:,0,0] #Divide loss by number of tokens\n",
    "            vr_loss = vr_loss.sum() / (guide_vr.size(0) * guide_vr.size(1))\n",
    "\n",
    "            #Actual key relation\n",
    "\n",
    "            actual_cross_attn_past_keys = actual_key_values[-2:][0]\n",
    "            actual_kr = self.reduce_actual_value_relations(actual_cross_attn_past_keys, vr_attn_mask)\n",
    "\n",
    "            kr_loss = self.kl_loss(actual_kr, guide_kr)\n",
    "            del actual_kr\n",
    "            kr_loss = (kr_loss * vr_attn_mask).sum(dim=[1,2,3])\n",
    "            kr_loss = kr_loss / vr_attn_mask.sum(dim=2)[:,0,0] #Divide loss by number of tokens\n",
    "            kr_loss = kr_loss.sum() / (guide_kr.size(0) * guide_kr.size(1))\n",
    "\n",
    "            del guide_vr\n",
    "            #del guide_attention\n",
    "            del guide_kr\n",
    "            total_attn_loss = total_attn_loss + kr_loss + vr_loss + attn_loss\n",
    "            #del attn_loss\n",
    "            del kr_loss\n",
    "            del vr_loss\n",
    "\n",
    "        total_attn_loss = total_attn_loss / len(current_stages)\n",
    "\n",
    "        loss = regular_loss * (1 - self.attention_alpha) + total_attn_loss * self.attention_alpha\n",
    "        return (loss, logits) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNCMsx0iihYQ"
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"/results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy='no',\n",
    "    learning_rate=1e-3, \n",
    "    weight_decay=0.01, \n",
    "    warmup_steps=100,\n",
    "    per_device_eval_batch_size=4, \n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    logging_steps=500,\n",
    "    group_by_length=True,\n",
    "    seed=seed,\n",
    "    data_seed=seed,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    optim='adamw_torch',\n",
    ")\n",
    "trainer = DistillationTrainer(\n",
    "    attention_alpha=1.0,\n",
    "    attention_temperature=1/16,\n",
    "    encoder_model=encoder_model,\n",
    "    teacher_model=teacher,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train['train'],\n",
    "    eval_dataset=data_train['val'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDsmyQuFmOBi"
   },
   "source": [
    "#Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzaNCKVNhu9Q"
   },
   "outputs": [],
   "source": [
    "kl_loss = torch.nn.KLDivLoss(reduction=\"none\")\n",
    "\n",
    "test_item = data_collator([data_train['train'][1]])\n",
    "test_item = { k: v.to(device) for k, v in test_item.items()}\n",
    "\n",
    "encoder_model.to(device)\n",
    "model.to(device)\n",
    "teacher.to(device)\n",
    "\n",
    "test_item.pop('length', None)\n",
    "token_transform = test_item.pop('token_transform', None)\n",
    "\n",
    "decoder_input_ids = test_item.pop('decoder_input_ids')\n",
    "teacher_decoder_input_ids = test_item.pop('teacher_decoder_input_ids', None)\n",
    "\n",
    "if teacher_decoder_input_ids is None:\n",
    "  teacher_decoder_input_ids = decoder_input_ids\n",
    "\n",
    "attention_mask = test_item.get('attention_mask')\n",
    "labels = test_item.pop('labels')\n",
    "decoder_attention_mask = labels != -100\n",
    "teacher_decoder_attention_mask = teacher_decoder_input_ids != 1\n",
    "\n",
    "layer_idx = -2\n",
    "\n",
    "with torch.no_grad():\n",
    "  encoder_output = encoder_model(**test_item, output_hidden_states=False, output_attentions=False)\n",
    "  out = model(**test_item, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, encoder_outputs=encoder_output, output_attentions=True, use_cache=True)\n",
    "  teacher_out = teacher(input_ids=teacher_decoder_input_ids, attention_mask=teacher_decoder_attention_mask, encoder_hidden_states=encoder_output[0], encoder_attention_mask=attention_mask, output_attentions=True, use_cache=True)\n",
    "\n",
    "attn_mask = torch.bmm(decoder_attention_mask.float().unsqueeze(2), attention_mask.float().unsqueeze(1)).cpu().detach()\n",
    "teacher_attn_mask = torch.bmm(teacher_decoder_attention_mask.float().unsqueeze(2), attention_mask.float().unsqueeze(1)).cpu().detach()\n",
    "vr_attn_mask = torch.bmm(attention_mask.float().unsqueeze(2), attention_mask.float().unsqueeze(1)).cpu().detach()\n",
    "\n",
    "actual_attentions = trainer.reduce_actual_attn_map(out.cross_attentions[layer_idx].cpu().detach(), attn_mask.bool())\n",
    "guide_attentions = trainer.reduce_guide_attn_map(teacher_out.cross_attentions[layer_idx].cpu().detach(), teacher_attn_mask.bool())\n",
    "\n",
    "actual_vr = trainer.reduce_actual_value_relations(out.past_key_values[layer_idx][-2:][1].cpu().detach(), vr_attn_mask.bool())\n",
    "guide_vr = trainer.reduce_guide_value_relations(teacher_out.past_key_values[layer_idx][-2:][1].cpu().detach(), vr_attn_mask.bool())\n",
    "\n",
    "if token_transform is not None:\n",
    "  guide_attentions = torch.matmul(token_transform.cpu().detach(), guide_attentions)\n",
    "\n",
    "  no_match_mask = torch.unsqueeze(token_transform.sum(dim=-1), dim=-1)\n",
    "  no_match_mask = torch.repeat_interleave(no_match_mask, attn_mask.shape[-1], dim=-1)\n",
    "  attn_mask = attn_mask * no_match_mask.cpu().detach()\n",
    "\n",
    "plt.imshow(guide_attentions[0], aspect=\"auto\", cmap=\"viridis\", vmax=0.01)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(actual_attentions[0].exp(), aspect=\"auto\", cmap=\"viridis\", vmax=0.01)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(guide_vr[0][0], aspect=\"auto\", vmax=0.3)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(actual_vr[0][0].exp(), aspect=\"auto\", vmax=0.3)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(attn_mask[0], aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "vr_loss = kl_loss(actual_vr, guide_vr)\n",
    "vr_loss = (vr_loss * vr_attn_mask).sum(dim=[1,2])\n",
    "vr_loss = vr_loss / vr_attn_mask.sum(dim=1)[:,0] #Divide loss by number of tokens\n",
    "vr_loss = vr_loss.sum() / guide_vr.size(0)\n",
    "\n",
    "print(\"VR loss: \", vr_loss)\n",
    "\n",
    "attn_loss = kl_loss(actual_attentions, guide_attentions)\n",
    "attn_loss = (attn_loss * attn_mask).sum(dim=[1,2])\n",
    "attn_loss = attn_loss / attn_mask.sum(dim=1)[:,0] #Divide loss by number of tokens\n",
    "attn_loss = attn_loss.sum() / guide_attentions.size(0) # to have batch_mean\n",
    "\n",
    "print(\"Attn loss: \", attn_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_tGtblSmB6V"
   },
   "source": [
    "#Upload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niH7QQkeqHjc"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ym99rvPfqI6C"
   },
   "outputs": [],
   "source": [
    "#for mBART:\n",
    "#reassign encoder\n",
    "model.model.encoder = encoder_model\n",
    "model.push_to_hub(\"josh-oo/mbart-ts-distil\", commit_message=\"Distilled layers: [r, r, -1] (0.75 vr/kr)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DkEQr5AmwPv"
   },
   "outputs": [],
   "source": [
    "#for mBART + GPT-2\n",
    "model.encoder = None\n",
    "model.push_to_hub(\"josh-oo/calibrated-decoder\", commit_message=\"Distilled layers: [r, r, -1] (0.75 vr/kr)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-wKBoenmE7o"
   },
   "source": [
    "##Auto Disconnect from Colab to Save Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8SaBj7gm4N5"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0orM464jnYIx",
    "r9hxuw9Bne24"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
